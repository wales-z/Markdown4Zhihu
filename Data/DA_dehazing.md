# 【论文翻译】DA_dahazing: Domain Adaptation for Image Dehazing

## 摘要

- 近年来，使用基于学习的方法进行图像去雾已经达到了最先进的性能。

- 然而，大多数现有方法在合成模糊图像上训练去雾模型，由于域迁移（domain shift)，这些模型面对真实的模糊图像泛化（generalize)能力不强。为了解决这个问题，我们提出了一种领域适应范式(范例？paradigm)，它由一个图像翻译模块和两个图像去雾模块组成。


- 具体来说，我们首先应用双向翻译网络，通过将图像从一个域转换到另一个域来缩小合成域和真实域之间的差距。然后，我们使用**翻译前后**的图像来训练这两个具有一致性约束的图像去雾网络。在这一阶段，我们利用清晰图像的属性（例如，暗通道先验(prior)和图像梯度平滑）将真实的模糊图像纳入到去雾训练中，以进一步提高域适应性。


- 通过以端到端的方式训练图像翻译和去雾网络，我们可以同时获得更好的图像翻译和除雾效果。在合成图像和真实世界图像上的实验结果表明，我们的模型性能优于目前最先进的除雾算法。

## 1 介绍

- 单个图像去雾的目的是从有雾的输入中恢复干净的图像，这对于后续的高级任务（例如对象识别和场景理解）必不可少。因此，在过去的几年中，它已经在视觉界引起了极大的关注。

- 根据物理散射模型[21，23，18]，通常将雾化过程公式为：
  $$
  I(x)=J(x)t(x)+A(1-t(x))\tag1
  $$
  
  其中 $I(x)$ 和 $J(x)$ 代表有雾图像和干净图像， $A$ 是全局大气光（global atmospheric ligt)，$t(x)$ 是介质传输图（transmission map).  $t(x)$ 可以表示为 $t(x)=e^{\beta d(x)}$，其中$d(x)$ 和 $\beta$ 分别表示景深（scene depth)和大气散射参数（atmosphere scattering parameter）。给定有雾图像 $I(x)$ ，大多数去雾算法都是尽量去估算 $t(x)$ 和 $A$。

- 然而，从模糊图像估计透射图通常是**不适定问题**（ill-posed problem)。早期的基于先验的方法尝试通过利用清晰图像的统计特性（例如，暗通道先验[9]和色线(color line)先验[8]）来估计透射图。不幸的是，这些图像先验很容易与实践不一致，这可能导致不正确的传输近似值（transmission approximations）。因此，恢复的图像的质量是不太理想的。

- 为了解决这个问题，卷积神经网络（CNN)开始被用于估计传输[4、26、35]或直接预测清晰的图像[12、27、16、25]。这些方法有效且优于基于先验的算法，并具有显着的性能提升。但是，基于深度学习的方法需要依赖大量真实的有雾图像及其无雾的对应图像进行训练。而通常，在现实世界中获取大量这样的真实图像是不切实际的。因此，大多数除雾模型都依靠合成去雾图像的数据集进行训练。但是，由于域迁移问题，从合成数据中学到的模型通常无法很好地泛化到实际数据。

- 为了解决上面这个问题，我们提出了一种用于单图像去雾的域自适应框架。框架包括两部分，称为图像翻译模块和两个与域相关的去雾模块（一个用于合成域，另一个用于真实域）。

  - 为了减少域之间的差异，我们的方法首先使用双向图像翻译网络将图像从一个域翻译到另一个域。由于图像雾度是一种噪声并且高度不均匀，取决于景深，因此我们将深度信息纳入转换网络，以指导合成图像翻译为真实有雾图像的翻译过程。

  - 然后，域相关的去雾网络将包括原始图像和翻译后图像在内的该域的图像作为输入以执行图像去雾。此外，我们使用一致性损失（consistency loss）来确保两个去雾网络生成一致的结果。

  - 在此训练阶段，为了进一步提高网络在真实域中的泛化能力，我们将真实的有雾图像纳入到训练中。我们希望真实的有雾图像的去雾结果能够具有清晰图像的一些特性，例如暗通道先验和图像梯度平滑。

  - 我们以端到端的方式训练图像翻译网络和除雾网络，以便它们可以互相改进。
    如图1所示，与最近EPDN的去雾效果相比，我们的模型产生的图像更清晰[25]。

    ![figure1](DA_dehazing\figure1.png)

<center>图1 真实有雾图像的去雾结果</center>

- 我们将本文的贡献总结如下：
  - 我们提出了一种用于图像去雾的端到端域自适应框架，该框架有效地减小了合成和真实世界有雾图像之间的差异e
  - 我们的结果表明将真实的有雾图像合并到训练过程中可以提高去雾性能。
  - 我们在合成数据集和真实世界的有雾图像上进行了大量实验，证明了所提出的方法的去雾性能优于目前最先进的去雾方法。

## 2 相关工作

本节简要讨论与我们的工作有关的单个图像去雾方法和域自适应方法。

### 2.1 单图像去雾

- **基于先验的方法。**基于先验的方法借助清晰图像的统计信息来估计介质传输图和大气光强度。在这方面的代表性作品包括[29、9、39、8、2]。具体来说，Tan [29]提出了一种用于图像去雾的对比度最大化方法，因为它观察到清晰的图像往往比其有雾的对应图像具有更高的对比度。He等人[9]利用暗通道先验（DCP）来估计介质传输图，该图基于以下假设：在至少一个颜色通道中，无雾斑块（haze-free patches)中的像素值接近零。后续工作提高了DCP方法的效率和性能[30，22，17，24，34]。此外，在[39]中采用衰减（attenuation）先验来恢复有雾图像的深度信息。 Fattal [8]使用色线假设来恢复场景传输（scene transmission)，他断言小图像块的像素呈现一维分布。同样，Berman等人[2]假设数百种不同的颜色可以很好地逼近清晰图像的颜色，然后基于此先验进行图像去雾。尽管已经证明这些方法对于图像去雾是有效的，但是由于假定的先验并不适合于所有真实图像，因此它们的性能固有地受到限制。

- **基于学习的方法。**随着深度卷积神经网络（CNN）的进步以及大规模合成数据集的可用性（availability），近年来，数据驱动的图像去雾方法受到了广泛的关注。许多方法[4，26，35，12]直接利用深层CNN来估计介质传输图和大气光，然后根据**退化模型**（1）恢复清晰图像。蔡等人[4]提出了端到端的除雾模型DehazeNet，从有雾的图像中估计传输图。任等人 [26]利用一种从粗到精（coarse-to-fine)的策略来学习有雾的输入和介质传输图的映射。 Zhang 和 Patel [35]提出了一个密集连接的金字塔网络来估计介质传输图。 Li等人[12]提出了一个AOD-Net来估计重新构造的物理散射模型的参数，该模型综合了传输和大气光。而且，已经提出了一些端到端方法[27、16、25]，以直接恢复干净的图像，而不是估计介质传输图和大气光。任等人[27]采用门控融合（gate fusion）网络直接从有雾的输入中恢复干净的图像。 Qu等人[25]将图像去雾问题转化为图像到图像翻译问题，并提出了一种增强的pix2pix去雾网络。

- 但是，由于合成数据与真实数据之间的域差距，在合成图像上训练的基于CNN的模型在应用于真实域时往往会出现明显的性能下降。为此，李等人[14]提出了一种半监督除雾模型，该模型在合成和真实有雾图像上都经过训练，因此享有合成和真实有雾图像之间的域适应性。然而，仅将真实的雾度图像应用于训练并不能真正解决域偏移的问题。
  与上述方法不同，我们的模型首先应用图像翻译网络将图像从一个域翻译到另一域，然后使用翻译后的图像及其原始图像（合成图像或真实图像）在合成域和真实域上执行图像去雾处理。我们提出的方法可以有效地解决域移位问题。

### 2.2 域自适应性

- 域自适应旨在减少不同域之间的差异[1、6、20]。现有的工作要么做特征级别要么像素级的自适应工作。
  - 特征级适应方法旨在通过最小化最大平均差异[19]（maximum mean discrepancy）或在特征空间上应用对抗性学习策略[32，31]来调整源域和目标域之间的特征分布。
  - 另一研究重点是像素级自适应[3，28，7]。这些方法通过应用图像到图像的翻译[3，28]学习或样式迁移[7]（style transfer）方法来增加目标域中的数据，从而解决了域偏移问题。

- 最近，很多方法在许多视觉任务中共同执行特征级和像素级自适应，例如图像分类[10]，语义分割[5]和深度预测[37]。这些方法[5、37]通过图像到图像转换网络，以像素级自适应将图像从一个域转换到另一域，然后将翻译后的图像以特征级别的对齐方式输入到任务网络，例如CycleGAN [38]。
- 在我们的工作中，我们利用CycleGAN来使真实的有雾图像适应我们在合成数据上训练的去雾模型。此外，由于**深度信息**与图像雾度的形成密切相关，因此我们将深度信息纳入到翻译网络中，以更好地指导真实有雾图像翻译。

## 3 提出的方法

本节介绍了我们的域适应框架的详细信息。首先，我们概述了我们的方法，然后描述图像翻译模块和图像去雾模块的细节，最后，给出了用于训练网络的损失函数。

### 3.1 方法总览

- 给定合成数据集$X_s=\{x_s,y_s\}^{N_l}_{s=1}$和真实有雾图像集$X_R=\{x_r\}^{N_u}_{r=1}$，其中$N_l$和$N_u$分别表示合成图像和真实模糊图像的数量。我们的目标是学习一个单一图像去雾模型，该模型可以从真实的有雾图像中准确预测出清晰的图像。由于域移位，仅在合成数据上训练的除雾模型无法很好地推广到真实的模糊图像。
- 为了解决这个问题，我们提出了一种域自适应框架，该框架包括两个主要部分：图像翻译网络$G_{S→R}$和$G_{R→S}$，以及两个除雾网络$G_S$和$G_R$。图像翻译网络将图像从一个域翻译到另一个域，以减小它们之间的差异。然后，去雾网络使用翻译后的图像和源图像（例如，合成的或真实的）执行图像去雾。
- 如图2所示，所提出的模型将真实的模糊图像$x_r$和合成图像$x_s$及其对应的深度图像$d_s$用作输入。我们首先使用两个图像翻译器获得相应的转换图像$x_{s→r}=G_{S→R}(x_s,d_s)$和$x_{r→s}=G_{R→S}(x_r)$。然后，将$x_s$和$x_{r→s}$传递给$G_S$，将$x_r$和$x_{r→s}$传递给$G_R$进行图像去雾。

![figure2](DA_dehazing\figure2.png)

<center>图2 本文提出的用于图像去雾的域自适应框架的架构。</center>

### 3.2 图像翻译模块

- 图像翻译模块包括两个翻译器：从合成到真实的网络$G_{S→R}$和从真实到合成的网络$G_{R→S}$。 $G_{S→R}$网络以$（X_s，D_s）$作为输入，并生成样式与真实有雾图像相似的翻译图像$G_{S→R}（Xs，Ds）$。另一个翻译器$G_{R→S}$执行的是逆向的图像翻译。由于深度信息与雾度公式高度相关，因此我们将其纳入到生成器$G_{S→R}$中，以在实际情况下生成具有相似雾度分布的图像。

- 我们采用空间特征变换（SFT）层[33，15]将深度信息纳入到翻译网络中，这可以有效地融合深度图和合成图像中的特征。

- 如图3所示，SFT层首先应用三个卷积层以从深度图提取条件图$\phi$。然后将条件图(conditional maps）馈送到其他两个卷积层以分别预测调制参数$\gamma$和$\beta$。最后，我们可以通过以下方式获得输出移位特征（output shifted features）：
  $$
  SFT(F|\gamma,\beta)=\gamma⊙F+\beta\tag2
  $$
  其中$⊙$是按元素的乘法。在翻译器$G_{S→R}$中，我们将深度图作为指导，并使用SFT层来变换倒数第二层卷积层的特征。

  ![figure3](DA_dehazing\figure3.png)

  <center>图3 STL层的结构</center>

  如图4所示，翻译后，合成图像相对更接近真实世界的有雾图像。

![figure4](DA_dehazing\figure4.png)

<center>图4 在两个合成有雾图像上的翻译结果。</center>

- 我们在表1中展示了翻译器$G_{S→R}$的详细配置。我们还采用了CycleGAN [38]提供的架构，用于生成器$G_{R→S}$和鉴别器(discriminators)（$D^{img}_R$和$D^{img}_S$）。

### 3.3 去雾模块

我们的方法包括两个去雾模块$G_S$和$G_R$，分别在合成域和真实域上执行图像去雾。 $G_S$将合成图像$x_s$和转换后的图像$x_{r→s}$作为输入来执行图像去雾。 $G_R$则用$x_r$和$x_{s→r}$训练。对于这两个图像去雾网络，我们都使用标准的编码器-解码器体系结构，其跳过连接（skip connections）和侧输出（side outputs）同[37]。每个域中的除雾网络共享相同的网络架构，但具有不同的学习参数。

