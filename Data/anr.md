# ANR: Aspect-based Neural Recommender

## 摘要

在许多电子商务和评论网站（例如 Amazon 和 Yelp）上都可以轻松获得文本评论，它们是推荐系统的宝贵信息来源。 然而，并非评论的所有部分都同等重要，同样的词语选择可能会根据其上下文反映不同的含义。 在本文中，我们提出了一种新颖的端到端基于 aspect  的神经推荐系统 (ANR)，通过基于注意力的组件为用户和物品执行基于 aspect  的表示学习。 此外，我们通过基于神经共同注意机制估计 aspect 级别的用户和物品重要性来建模用户给物品评分背后的多方-面的 (multi-facted) 过程。 我们提出的模型同时解决了现有推荐系统的几个缺点，对来自 Amazon 和 Yelp 的 25 个基准数据集的彻底实验研究表明，ANR 显着优于最近提出的最先进的基线，如 DeepCoNN、D- Attn 和 ALFM。

## 1 引言

随着生活方式向日益数字化的转变，推荐系统在帮助消费者在各种选择中找到最佳产品或服务方-面发挥着关键作用。 一些最广泛使用和成功的推荐系统依赖于协同过滤 (CF) 技术，该技术利用过去的交互数据，如评分、购买日志或查看历史，对用户偏好和物品特征进行建模 [22]。 然而，矩阵分解 (MF) 等 CF 技术的一个主要限制是它无法为评分很少的用户提供可靠的推荐，或者推荐评分有限的物品，即现实世界推荐系统中众所周知的冷启动问题。

最近的推荐系统考虑了另一种有价值的信息来源，它可以在许多电子商务和评论网站（例如 Amazon 和 Yelp）中轻松获得：自由文本 (free-text) 评论。 通常情况下，用户会提供随附的评论来解释他们喜欢或不喜欢特定产品或服务的原因，即总体数字评分背后的原因。 例如，评论可能包括用户对物品各个 aspect 的意见，例如其价格、性能、质量等。事实上，评论提供的不仅仅是一种对隐式用户偏好或物品属性进行建模的途径。 这些评论中丰富的语义信息有助于我们理解用户如何对物品进行评分背后的多 aspect  过程，即影响用户偏爱一个物品而不是另一个物品的关键因素

由于其卓越的表示学习能力，深度学习技术已被广泛用于最近的SOTA的推荐系统，以使用评论内容构建潜在用户和物品表示。 这包括 DeepCoNN [44]、D-Attn [33] 和 TransNets [7] 等模型，所有这些模型都基于使用卷积神经网络 (CNN) [20] 将用户（和物品）评论编码到他们的对应的潜在 embedding。虽然这些提出的方法已被证明提供了良好的预测性能，但他们简单地为每个用户（和物品）提供单个低维潜在表示的方法本质上会受到限制：无法捕获用户和物品之间更细粒度交互。

直观地说，并非评论的所有部分都同等重要。例如，评论的某些部分可能是在描述电影的情节，甚至是书中的故事情节，而这些“细节”可能与用户的整体满意度无关。 一个常见的观察结果是，评论的每个部分都倾向于关注用户整体体验的不同方-面，例如餐厅的位置、服务人员的态度，甚至该餐厅供应的菜肴的口味。 通过关注这些显着因素，我们可以更好地推断特定用户的偏好（例如，用户 X 喜欢有户外座位的餐厅）和物品的属性 (properties)（例如 Y餐厅以其海鲜菜肴而闻名）。

然而，要对评论内容的丰富语义进行建模，必须超越表面级 (surface-level) 的单词表示。考虑以下包含“long”一词的两个句子：（1）“This laptop has a **long** battery life”，以及（2）“The laptop requires a **long** startup time”。 很明显，“long”这个词在第一句中对目标 aspect（或物品属性）具有积极的情绪，而在第二句中，同一个词对完全相同的物品表示负面情绪。 因此，一种灵活的词表示方案能够考虑这样的上下文信息 w.r.t. 任何给定的 aspect 都是可取的。

此外，在与这些物品的交互过程中，不同的用户可能会更多地强调不同 aspect  。 例如，一些用户可能喜欢某家特定餐厅的食物，而其他用户由于其舒适的氛围而经常光顾同一家餐厅。类似地，用户在选择恐怖片时可能会优先考虑故事情节，但在评估动作片时更关注演员阵容。 可以理解的是，每个 aspect 的重要性在很大程度上取决于用户和相关物品，并且能够捕捉用户和物品之间的这种动态和细粒度的交互对于确定为什么某些用户可能更喜欢一个物品而不是另一个是非常宝贵的。 在本文中，我们的目标是对这一关键信息进行建模以进行推荐。

本文的主要贡献总结如下：

- 提出了一种新颖的基于 aspect 的神经推荐系统，该系统通过设计一种注意力机制来关注这些评论的相关部分，同时学习任务的 aspect 表示，从而为用户和物品执行基于 aspect 的表示学习。 此外，我们使用共同注意的思想以联合方式估计 aspect  级用户和物品的重要性，这使我们能够对用户和物品之间的细粒度交互进行建模。据我们所知，这是第一篇提出端到端基于神经 aspect 的推荐系统的论文，该系统同时满足上述要求。
- 对来自 Amazon 和 Yelp 的 25 个基准数据集进行了广泛的实验，以针对多个最先进的 Baseline（例如 DeepCoNN [44]、D-Attn [33] 和 ALFM）评估我们提出的模型[10]。
- 我们调查了我们提出的模型中的不同组件如何提高其有效性。特别是，我们对模型自动学习的 aspect  进行了定性分析，无需任何外部监督。

## 2 相关工作

最近的工作 [1, 3, 4, 7, 24, 26, 33, 35, 40, 44] 都表明使用评论来提高推荐系统的性能和可靠性的重要性。 因此，我们专注于与我们的工作高度相关的几个关键领域：

（1）基于深度学习的推荐系统，

（2）基于 aspect 的推荐系统，以及

（3）神经注意力和共同注意力。

### 2.1 基于深度学习的推荐系统

近年来，深度神经网络已成功应用于各种任务，例如自然语言处理、计算机视觉和语音识别 [14]，通常在这些领域取得了最先进的性能。许多最近提出的推荐系统也转向了各种深度学习技术以处理文本信息，例如在 [23, 39] 中使用去噪自动编码器，在 [1, 3] 中使用循环神经网络 (RNN)，以及最值得注意的是，在 [7, 8, 19, 33, 44] 中使用卷积神经网络 (CNN) [20]，因为它在许多其他自然语言处理任务中取得了巨大成功 [11, 18]。

通常，这些方法试图利用神经网络强大的表示学习能力，从用户和物品的评论中学习潜在特征表示。但是，尝试将用户（或物品）的所有可用评论“压缩”为单个潜在表示可能并不理想。 除了可能丢失有用信息（由于此类模型中使用的池化技术）之外，还存在包含这些评论的不相关部分的固有风险，从而导致对用户（或物品）的嘈杂且可能不准确的表示。

此外，用户和物品之间的唯一交互发生在最终预测层，其中学习到的用户和物品嵌入用于使用 [7, 44] 中的因子分解机 (FM) [31]  、[8] 中的前馈神经网络，或简单地通过 [33] 中的内积等方法进行整体评分估计 。在这些模型中，很难提供令人信服的见解，以说明用户为何以这种特定方式对物品进行评分。

### 2.2 基于 Aspect 的推荐系统

在利用深度学习技术进行推荐之前，一个流行的研究方向集中在从这些文本评论中提取或学习 aspect 。
第一种基于 aspect 的推荐系统，如 EFM [43]、TriRank [16]、LRPPM [9] 和最近提出的 SULM [5]，依赖于外部情感分析 (SA) 工具 [30] 来分析 查看内容并揭示所提及的 aspect 以及他们的意见和/或情绪。 除了它们不是独立的这一事实之外，此类模型的性能在很大程度上取决于这些 SA 工具的质量，即它们能够从这些文本评论中提取此类信息的能力。

另一种基于 aspect 的系统 [10, 12, 40] 自动从评论内容中学习这些 aspect ，通常是通过使用生成性统计模型，如潜在狄利克雷分配 (LDA) [6, 42]。  JMARS [12] 和 FLAME [40] 都是集成的概率框架，它们将每个 aspect 表示为词汇表中单词的分布。 新提出的 ALFM [10] 包括一个 aspect 感知主题模型（ATM），该模型将每个 aspect 建模为同一组 $K$ 个潜在主题上的多项分布，其中每个主题都定义为词汇表上的多项分布。 ATM 的输出，即 aspect 级别的用户偏好和物品特征，随后被用作潜在因素模型的一部分，用于通过 MF 方法估计整体评分.

这些基于 aspect 的方法的一个关键优势是它们通常更加透明和直观，因为它们中的大多数都能够提供解释以支持他们的建议。 然而，现有的基于 aspect 的系统要么 (1) 依赖于外部工具或输入，要么 (2) 不强调评论的不同部分对整体满意度的贡献如何不同。 此外，他们没有同时考虑用户和物品的不同 aspect 级别的重要性，同时考虑目标用户和相关物品（必要时）。

### 2.3 神经注意力 & 共同注意力

基于人类视觉注意的思想，神经注意机制是深度学习领域最令人兴奋的发展之一，并已成功应用于机器翻译和抽象摘要等众多机器学习任务 [2  , 32, 37]。 最近，它也被用于各种推荐系统 [8, 33, 36]。 本质上，它使神经网络能够专注于输入的选择性部分，例如图像中的某个区域，甚至是文本文档中的特定单词/句子。

例如，如果我们试图根据某个餐厅的价格来确定其适合度，则并非其用户评论集中的所有词都同等重要。 几乎本能地，我们会将注意力转向这些评论中的信息性词的子集，例如expensive, cheap, costly, affordable 等。这是我们提出的模型如何能够使用完全数据驱动的方法从相应的文本内容中自动推导出 aspect 级表示的核心思想。基本上，该模型通过神经注意力机制学习识别与给定某些目标 aspect 高度相关的词汇表的子集。

一种密切相关的技术是神经共同注意力 [25, 41]，它可以粗略地描述为 pairwise 神经注意力的一种形式。在某些情况下，联合推理一对相关实体的注意力可能是有益的，例如 [25] 中视觉问答任务的图像和问题之间的注意力。 神经协同注意机制背后的基本思想是对一个实体（例如图像）的注意是通过其他实体的表示（例如问题）来学习的，反之亦然。

对于我们的模型，我们扩展了双向 (two-way) 神经注意力的这种特殊思想，用于估计 aspect 级别的用户和物品重要性，通过了解当前用户-物品对的能力来增强它。aspect 级别的物品表示被用作影响 aspect 级别用户重要性的学习的上下文，反过来，aspect 级别的物品重要性以 aspect 级别的用户表示为条件。 换句话说，我们提出的模型在推断每个 aspect 对用户的重要性时考虑了目标物品，反之亦然

## 3 所提出的模型

在本节中，我们将介绍我们提出的基于 Aspect 的神经推荐系统 (ANR)，这是一种神经推荐系统，旨在在 aspect 级别捕获用户和物品之间更细粒度的交互。 首先，我们指定问题设置和使用的关键符号，并概述我们的架构以及一些关键组件背后的动机。

接下来，我们详细描述了我们用于学习 aspect 级用户（和物品）表示的基于注意力的模块。 接下来，我们将展示我们的基于共同注意的模块，用于动态推断任何给定用户-物品对的 aspect 级别重要性，以及如何有效地结合 aspect 级别表示和重要性来推断整体评分 . 最后，我们将介绍 ANR 的模型优化细节

### 3.1 问题设定

考虑由评分和评论构成的语料库 $\cal D$，对于一组物品 $\cal I$ 和一组用户 $\cal U$，每个用户-物品交互可以表示为一个元组 $(u,i,r_{u,i} ,d_{u,i})$ 其中 $r_{u,i}$ 是一个数字评分，表示用户 $u$ 对物品 $i$ 的总体满意度，并且 $d_{u,i}$ 是对应的文本评论。主要目标是估计所有未见过 (unseen) 的用户-物品对的评分 $\hat r_{u,i}$，即给定用户 $u$ 对他/她之前未与之交互的物品 $i$ 的未知评分。 表 1 总结了本文其余部分使用的关键符号。

![image-20211021230009077](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211021230009077.png)

> 注释1：除非另有说明，否则我们都将向量以粗体小写表示，矩阵或高维张量以粗体大写表示

### 3.2 ANR 总览

图 1 显示了我们提出的模型的整体架构。 类似于 [33, 44]，我们将用户文档 $D_u$ 和物品文档 $D_i$（即用户 $u$ 撰写的评论集合和为物品 $i$ 撰写的评论集合）分别作为网络的输入。 由于用户和物品的建模过程是相同的，我们专注于说明给定用户的过程。 应该注意的是，用户和物品文档的构建限定为来自训练集的评论集合，即它们不包括来自验证集或测试集的任何评论。

![image-20211022165506510](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211022165506510.png)

【图 1】

#### 3.2.1 嵌入/Embedding 层

首先，用户文档 $D_u$ 通过嵌入层转换为矩阵 $\bold M_u \in \mathbb R^{n\times d}$，其中 $n$ 是 $D_u$ 中的词数，$d$ 是每个词嵌入向量的维数。 基本上，嵌入层在一个共享嵌入矩阵 $f : \cal V \rightarrow \mathbb R^d$ 中执行查找操作，它将词汇表 $V$ 中的每个单词映射到其对应的 $d$ 维向量。 嵌入矩阵可以使用已在大型语料库上预训练的词向量初始化，例如 word2vec [28] 或 GloVe [29]，这有助于更好地表示用户（和物品）文档的语义。 与依赖词袋假设的基于主题建模的方法不同，单词的顺序和上下文保留在嵌入文档中。

#### 3.2.2 基于 aspect 的表示学习

例如，考虑到餐馆领域， aspect 集合 $\cal A$ 可能包括诸如价格、质量、服务、位置等 aspect 。换句话说， 对于一个给定的领域，aspect 可以定义为围绕 (encompass) 物品的某个特定方-面的属性的高级语义概念。 对于餐厅，服务这个 aspect 可以包含诸如 {员工、等待时间、预订、代客泊车等属性}。

给定嵌入的用户文档表示 $\bold M_u$，我们的目标是推导出关于一组 $K$ 个域相关 (domain-dependent) 的 aspect $\cal A$ 的一组 aspect 级用户表示 $\bold P_u = \{\bold p_{u,a} | a \in \cal A\}$  。
直观地说，评论 $d_{u,i}$ 描述了用户 $u$ 基于这个 aspect 集（或可能的子集）对物品 $i$ 的意见 (opinion) 。 因此，用户文档 $D_u$ 涵盖了用户 $u$ 对 $\cal A$ 的意见，汇总了他/她之前与之交互的所有物品。类似地，物品文档 $D_i$ 描述了物品 $i$ 的属性，汇总了所有评论过它的用户。

我们的假设是，给定足够的数据，通过学习关注每个用户（或物品）文档中与 aspect 相关的词一个子集，我们可以学习这组 aspect $\cal A$，以及每个 aspect $ a \in \cal A $的 aspect 级用户（或物品）表示。 在本文中，我们提出了一种新的基于注意力的 aspect-aware 的组件来学习这些 aspect 级别的表示，细节在第 3.3 节中介绍。

#### 3.2.3 Aspect 重要性

不同用户对物品的不同 aspect 有不同偏好的情况并不少见。 此外，对于给定的用户，他/她的 aspect 偏好可能会根据目标物品而改变。 例如，用户在选择手机时可能会关注价格和美观，而在购买笔记本电脑时可能更关注性能和便携性。 同样，同一物品可能对两个不同的用户产生不同的吸引力。 例如，一些用户可能喜欢某家特定餐厅的食物，而另一个用户由于其舒适的氛围而经常光顾同一家餐厅。

此外，这些 aspect 通常不会被单独评估。例如，如果手机的质量和性能超出他/她的预期，用户可能愿意忽略其高昂的价格，即使历史数据可能表明该用户通常更喜欢更便宜的手机。

因此，我们的新想法是，通过动态估计每个用户-物品对 (pair) 的用户和物品 aspect 的重要性，在 aspect 级别对用户和物品之间丰富而复杂的交互进行建模，而非具静态的用户和物品 aspect 的重要性。 在本文中，我们提出了一种新的基于共同注意力的组件，该组件能够考虑这些重要的观察结果以进行推荐，详细信息在第 3.4 节中介绍

### 3.3 基于 Aspect 的表示学习

在深入研究我们提出的基于 aspect 的表示学习方法的具体细节之前，我们强调了一些旨在通过该组件捕捉的重要直觉 (intuition)。

**直觉 1**：并非评论（或文档）中的所有单词都同等重要，并且每个文档单词对于正在考虑的 aspect 的重要性各不相同。 一般来说，评论往往包括对目标物品的多个 aspect 的意见，在学习给定 aspect 的 aspect 级表示时，我们应该能够专注于审查（或文档）的特定子部分 (subpart)。

**直觉2**：同一个词在同一领域的两个不同 aspect 的情感极性可能完全不同。例如，“This phone has a **high** storage capacity”和“This camera captures **high** quality images”这两个句子中的“high”一词带有对目标 aspect （或物品属性）的积极情绪。 另一种情况 ，考虑“The price is way too **high**”和“This computer has extremely **high** power comsumption”这两个句子，同一个词实际上反映了一种负面情绪。 事实上，许多这些带有情感的词往往会根据所考虑的 aspect 指示不同的极性，这应该在 aspect 级表示中捕获。

**直觉 3**：众所周知，与 aspect 相关的词（例如价格、味道、氛围）和它们的情感词（例如昂贵、美味、惊人）通常很接近 [17]。这意味着我们可以通过查看其周围的单词，即通过考虑局部上下文窗口来更好地推断文档中单词的重要性

现在，我们描述 aspect 级别的用户表示，即对给定用户 $u$ 和 aspect $a \in \cal A$ 可以得到  $\bold p_{u,a}$ 。 由于词汇表 $\cal V$ 中的所有单词在 $K$ 个 aspect 之间共享相同的 $d$ 维向量，我们使用特定于 aspect 的词投影矩阵^4^ $\bold W_a \in \mathbb R^{d \times  h_1}$ 来允许词表示关于目标 aspect $a$的变化 wrt （直觉2）。 形式上为：
$$
\bold M_{u,a}[i]=\bold M_{u}[i] \bold W_a
\tag1
$$

> 注释4：请注意，$h_1$ 是一个超参数，它允许定义用于 aspect 级表示的潜在因子的数量，而不受原始词嵌入大小的限制

其中 $\bold M_u[i]$ 是 $\bold M_u$ 中第 $i$ 个词的原始 $d$ 维词嵌入，$\bold M_{u,a}[i]$ 是特定于 aspect 的词表示，而 $\bold M_{u,a} \in \mathbb R^{n \times h_1}$ 是用户 $u$ 和aspect $a$ 的特定于 aspect 的文档嵌入。 该投影的结果是 $\mathbb R^{K \times n \times h_1}$ 中的 $K$ 个不同 aspect 的张量。

每个 aspect  $a \in \cal A$ 表示为一个长度为 $c \times h_1$ 的嵌入向量 $\bold v_a ∈ \mathbb R^{(c \times  h_1)}$，其中 $c$ 是一个超参数，它决定了局部上下文窗口的宽度（根据词的数量）。

为了计算第 $i$ 个文档词在这个特定于 aspect 的嵌入子空间中的重要性，我们考虑一个以它为中心词的局部上下文窗口：
$$
\bold z_{u,a,i}=(\bold M_{u,a}[i-c/2];...;\bold M_{u,a}[i];...;\bold M_{u,a}[i+c/2])
\tag2
$$
其中 $(·;·)$ 是拼接 (concatenation) 运算符。 我们通过取内积和 softmax 函数来计算第 $i$ 个单词的注意力分数：
$$
{\rm attn}_{u,a}[i] = {\rm softmax} (\bold v_a (\bold z_{u,a,i})^T)
\tag3
$$
其中 ${\rm softmax}(w_i) = exp(w_i) / \sum_j exp(w_j)$，并且 ${\rm attn}_{u,a}$ 是在用户 $u$ 关于 aspect $a$ 的文档词上定义的软注意力向量（即概率分布）。从本质上讲，文档中第 $i$ 个单词的重要性取决于单词本身及其周围的单词（直觉 3）。 考虑到学习到的文档中每个单词的重要性（直觉 1），可以基于以下加权和导出 aspect 级用户表示
$$
\bold p_{u,a}=\sum^n_{i=1}({\rm attn}_{u,a} \bold M_{u,a}[i])
\tag 4
$$
可以按照类似的方式，按照等式（1）到（4）获得物品 $i$ 和 aspect  $a$ 的 aspect 级物品表示 $\bold q_{i,a}$ 。

此外，对于每个 aspect  $a \in \cal A$，我们共享用户和物品的 aspect 嵌入向量 $\bold v_a$ 和 aspect 特定的词投影矩阵 $\bold W_a$（即每个 aspect  $a$ 的 aspect 级用户和物品表示驻留在相同的 aspect 特定特征空间中）。共享特定于 aspect 的参数使我们能够更好地学习用户和物品文档中的 aspect 之间的映射，同时减少模型中可训练参数的数量。 我们将基于 aspect 的表示学习层的参数集表示为 $\Theta_{ARL} = \{\bold v_a,\bold W_a|a\in \cal A \}$ ^5^

> 注释5：对于每个aspect $a$，aspect 嵌入向量 $\bold v_a$ 和特定于 aspect 的投影矩阵 $\bold W_a$ 都使用均匀分布 ${\cal U}(-0.01, 0.01)$ 随机初始化。本质上，基于 aspect 的表示学习是以数据驱动的方式进行的，没有任何外部监督。

### 3.4 Aspect 重要性估计

一个直接的解决方案是我们可以尝试分别估计用户和物品 aspect 的重要性。 然而，这将导致“静态”的用户和物品 aspect 的重要性，即：用户 aspect 的重要性实际上并未考虑实际感兴趣的物品，反之亦然。 换句话说，用户（物品）aspect 的重要性在所有可能的物品（用户）中保持固定，并且对于任何给定的用户-物品对都不是最佳的，因为它不是专门为相关用户和物品派生 (derive) 的。

为此，我们建议以联合方式学习用户和物品 aspect 的重要性。 在学习用户 aspect 重要性时， aspect 级别的物品表示用作上下文，类似地， aspect 级别的用户表示可以用作学习物品 aspect 重要性时的上下文。 该层的输出将是一个 $K$ 维向量，指示每个 aspect 对用户的重要性，以及相应的物品的 $K$ 维向量

为了在计算用户 aspect 重要性时合并 aspect 级别的物品表示（反之亦然），我们需要知道目标用户和物品在 aspect 级别如何匹配。 首先，使用 aspect 级用户表示 $\bold P_u \in \mathbb R ^{K \times h_1}$ 和物品表示 $\bold Q_i \in \mathbb R ^{K \times h_1}$我们可以得到一个 aspect 级亲和度 (affinity) 矩阵 $\bold S$ 如下
$$
\bold S=\phi(\bold P_u \bold W_s \bold Q^T_i)
\tag5
$$
其中 $\bold W_s \in \mathbb R^{h_1 \times h_1}$ 是一个可学习的权重矩阵， $\phi(x) = \max(0,x)$ 是 ReLU 函数，亲和度矩阵 $\bold S \in \mathbb R^{K \times K}$ 中的每个条目表示相应的 aspect 级用户和物品表示之间的亲和度（或共享相似度） 对 (pair)。我们在图 2 中提供了具有 $K$ 个 aspect 的亲和力矩阵的说明。

![image-20211022174400117](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211022174400117.png)

【图 2】

接下来，按照 [25]，我们使用亲和度矩阵 $\bold S$ 作为特征来估计 aspect 级别的用户和物品重要性：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211022174528180.png" alt="image-20211022174528180" style="zoom: 67%;" />

其中 $\bold W_x , \bold W_y \in \mathbb R ^{h_1 \times h_2}$ 和 $\bold v_x , \bold v_y \in \mathbb  R^{h_2} $ 是可学习的参数。  $\beta_u \in \mathbb R^K$ 和 $\beta_i \in \mathbb R^K$ 分别是用户 $u$ 和物品 $i$ 在 $K$ 个 aspect  的集合 $\cal A$ 上估计的 aspect 重要性。
基本上，我们在计算 $\bold H_u$ 和 $\bold H_i$ 时同时考虑用户表示 $\bold P_u$ 和物品表示 $\bold Q_i$。 考虑到 aspect 级表示可能在 (1) 用户和物品、(2) 两个不同用户和 (3) 两个不同物品之间存在显着差异，我们发现这些额外的隐藏层通过以下方式提高了模型性能：允许它更好地估计任何给定的的用户-物品对的、pairwise的、 aspect 级别的重要性。

正如前面所强调的，我们专门设计了这个组件来考虑目标用户和物品，从而能够同时对用户和物品个性化的 aspect 级别重要性的这种估计。我们将 aspect 重要性估计层 (Aspect Importance Estimation layer) 的参数集合表示为 $\Theta_{AIE} = \{\bold W_s , \bold W_x , \bold W_y, \bold v_x , \bold v_y \}$。

### 3.5 模型推理和优化

我们现在描述图 1 所示的 User-Item Rating Prediction 组件。通过将用户和物品 aspect 级别的表示  $\bold P_u$  , $\bold Q_i$ 与 aspect 重要性 $\beta_u$ , $\beta_i$ 相结合，可以推断出任何用户-物品对的总体评分如下：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211022175533673.png" alt="image-20211022175533673" style="zoom: 50%;" />

其中 $b_u$ 、$b_i$ 、$b_0$ 分别是用户、物品和全局偏置（如在传统的潜在因子模型中）。模型优化过程可以看作是一个回归问题，完整的模型参数集合 $\Theta = \{\Theta_{ARL},\Theta_{AIE}, b_u, b_i, b_0\}$ 可以使用反向传播技术以标准均方误差 (MSE) 作为损失函数来学习

#### 3.5.1 预训练

在 [13] 中已经表明，神经网络的性能可能对参数的初始化方式相当敏感。 对于我们提出的模型，aspect 重要性估计组件完全基于来自先前基于 aspect 的表示学习层的输出 $\bold P_u$ ,$\bold Q_i$ ，即它隐式地依赖于参数集 $\Theta_{ARL} = \{\bold v_a, \bold W_a | a\in \cal A\}$。 因此，我们采用了一个使用简化模型的预训练阶段来获得 $\Theta_{ARL}$ 的良好初始化。 我们分别用两个前馈神经网络为用户和物品替换了 aspect 重要性估计组件。 用户（物品）网络将 aspect 级用户（物品）表示的串联作为输入，并生成抽象的用户（物品）表示。 然后将这些抽象的用户和物品表示连接起来，并通过另一个前馈层用于预测总体评分 $\hat r_{u,i}$。这个简化的模型不考虑用户和物品之间的 aspect 级交互，它使用反向传播方法和 MSE 损失函数以类似的方式训练。

#### 3.5.2 泛化

许多现有的工作发现深度学习模型往往会受到过拟合的影响。 为了提高泛化性能，我们采用了 dropout 技术 [34]，该技术广泛用于现有的推荐神经模型 [7, 8, 33, 44]。 对于每个 aspect 级别的表示，它是一个潜在因子的 $h_1$ 维向量，在训练阶段随机丢弃该向量的 $\rho \%$。此外，我们将 L2 正则化应用于公式（8）中的用户和物品偏置。

### 4 实验

我们使用来自 Yelp 和 Amazon 的公开可用数据集，针对几种最先进的 baseline 方法评估我们提出的模型。 在本节中，我们描述使用的数据集，介绍baseline 方法，详细说明实验设置，并展示实验结果。

#### 4.1 数据集

对于 Yelp，我们使用 Yelp 数据集挑战^6^的最新版本（第 11 轮），其中包含对 4 个国家/地区的本地企业的评级和评论。 至于亚马逊，我们使用 [15, 27] 中的亚马逊产品评论^7^，它已经被组织成 24 个单独的产品类别。

>注释6：https://www.yelp.com/dataset/challenge
>
>注释7：http://jmcauley.ucsd.edu/data/amazon/

对于 Yelp 数据集和来自亚马逊的 3 个较大的数据集（即书籍、电子产品和服装、鞋履和珠宝），我们随机抽取了 5000000 个用户-物品交互进行实验。之后，类似于 [7, 8, 33, 44]，我们使用 80:10:10 的比例将这 25 个数据集中的每一个随机划分为训练、验证和测试集。 根据 [7, 26, 44]，我们直接使用这些数据集。 具体来说，我们选择不采用 [8, 10, 33] 中使用的“5-core setting”，即每个用户和物品至少有 5 个评分/评论，因为它使数据稀疏性问题变得微不足道，而在现实世界的推荐系统中不可避免。 表 2 显示了所用数据集的统计数据。

![image-20211022194340395](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211022194340395.png)

> 注释8：这些数据集的平均稀疏度为 99.9985%，每个用户和物品（跨所有数据集）的平均评分/评论数分别为 1.91 和 12.12。

### 4.2 Baseline 方法

我们将我们提出的方法与 3 种SOTA的 baseline 方法进行比较，这些方法利用评论信息来提高整体推荐性能。

(1) Deep Cooperative Nerual Networks (**DeepCoNN**) [44]：这是一种SOTA的神经推荐模型，它使用卷积架构从相应的评论中导出潜在的用户和物品表示。 用户和物品表示被拼接 (concatenate) 起来并用作因子分解机（FM）[31] 的输入，用于整体评级预测。大量的实证评估表明，DeepCoNN 远远优于经典的推荐方法，例如矩阵分解 (MF) [22]、潜在狄利克雷分配 (LDA) [6] 和隐藏因子作为主题 (HFT) [26]。

(2) Dual Attention-based Model (**D-Attn**) [33]：与 DeepCoNN 类似，D-Attn 依靠卷积神经网络 (CNN) 来学习用户和物品表示。 关键区别在于，在卷积层之前，D-Attn 结合了基于局部和全局注意力的模块，分别用于从评论中选择局部和全局信息词。 然而，D-Attn 不是使用 FM，而是简单地使用用户和物品表示的内积进行评分预测。

(3) Aspect-aware Latent Factor Model (ALFM) [10]：ALFM 是最先进的基于 aspect 的推荐系统，它不依赖于外部情感分析工具。 作者设计了一个 aspect 感知主题模型 (ATM) 来将每个 aspect  $a \in \cal A$ 表示为基于评论内容的潜在主题的分布。 然后将来自 ATM 的输出与 ALFM 结合，后者通过对评级使用 MF 方法将潜在因素与相同的 aspect 集合 $\cal A$ 相关联。

应该注意的是，所有三种 baseline 方法都是最近提出的，其中，已证明优于许多其他竞争激烈的推荐方法 [7, 24, 35, 38, 39]。

### 4.3 实验设置

首先，所有评论都使用 NLTK^9^ 进行标记，我们保留 50000 个最常用的单词作为每个数据集的词汇表 $\cal V$。
对于 ALFM，我们使用作者提供的代码，并遵循论文中报告的超参数设置和优化方法。ALFM 中使用的 aspect 和潜在主题的数量都设置为 5。尽管 [10] 仅使用 5 个潜在因素进行模型比较，但他们的超参数研究发现，更多的潜在因子通常会带来更好的性能。 因此，我们使用验证集为每个数据集在 {5, 10, 15, 20, 25} 中选择最佳潜在因子数。

> 注释9：https://www.nltk.org/

我们使用 PyTorch 实现了神经推荐模型，即 DeepCoNN、D-Attn 以及我们提出的方法。
我们设置输入用户和物品文档的长度，即 $|D_u|$ 和 $|D_i|$ 为 500。我们的模型和 DeepCoNN 使用在 Google News [28] 上训练的 300维 词嵌入，而 D-Attn 使用使用 GloVe [29] 在维基百科上训练的 100维词嵌入（我们尝试使用 D-Attn 具有相同的 300 维嵌入，但它在多个数据集上始终会降低其性能）。 我们将 [33, 44] 中报告的设置重用于超参数，例如卷积滤波器的数量和大小、用于全连接层的因子数量以及激活函数。 对于 DeepCoNN，基于使用验证集的网格搜索，我们将 dropout rate 设置为 $0.5$，将 FM 中使用的因子数量设置为 10，因为这些值在论文中未指定。 为了与 ALFM 进行公平比较，我们在模型中使用相同数量的 aspect ，即 $|{\cal A}|  = K = 5$。ANR 的其他超参数，例如局部上下文窗口 $c$ 的宽度、潜在因子 $h_1,h_2$ 的数量和dropout rate $\rho$ 分别设置为 3、10、50 和 0.5。 所有 3 个神经模型都使用 Adam [21] 进行训练，使用初始学习率为 0.002、batch size为 128 ，使用 MSE 损失。

遵循 [33, 44]，我们使用标准均方误差 (MSE) 作为评估指标。 所有实验都重复 5 次，我们报告了验证阶段 MSE 最低时获得的（平均）测试 MSE

### 4.4 结果和讨论

表 3 显示了我们在所有 25 个数据集上的实验结果。我们观察到，基于配对样本 t 检验，使用每个模型 5 次独立运行的结果，ANR 在所有 3 种最先进的 baseline 方法上都取得了统计上的显着改进。

![image-20211022200114871](C:/Users/Wales-Z/AppData/Roaming/Typora/typora-user-images/image-20211022200114871.png)

【表3】

接下来，我们注意到诸如 ALFM 和 ANR 之类的 aspect 感知推荐方法始终优于 DeepCoNN 和 D-Attn。
我们认为这可以归因于 DeepCoNN 和 D-Attn 将用户（和物品）文档“压缩”成单个表示（即向量），因此，用户和物品之间唯一的“交互”发生在预测层，即当使用用户和物品表示来预测整体评级时。 换句话说，他们无法捕捉这些用户-物品交互中涉及的多方-面决策过程。由于使用卷积层作为编码器，DeepCoNN 和 D-Attn 都具有相似的模型架构，而且看起来 D-Attn 使用附加的基于局部和全局注意力的模块会表现得更好。 然而，D-Attn 之前是使用更密集的 5-core 设置 [33] 进行评估的，由于数据稀疏性，它似乎表现不佳，这在我们的实验设置中很明显。

最后，尽管 ALFM 尝试在其框架中利用评论内容，但他们使用主题建模方法来实现。一个主要的缺点是，ALFM 提出的 Aspect-aware Topic Model (ATM) 在从评论中推断用户和物品偏好时没有考虑评分信息； 当 ALFM 使用 MF 方法学习潜在用户和物品表示时，不使用评论内容。 换句话说，与我们提出的方法不同，ALFM **分离地** (separately) 使用评论内容和评分信息。

## 5 模型分析

在本节中，我们将检查关键超参数对模型性能的影响。 此外，我们通过对学习 aspect 的定性分析和消融研究，提供了我们模型内部运作的一瞥。

### 5.1 参数敏感性

