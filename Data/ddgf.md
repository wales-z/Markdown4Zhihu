迁移学习之：Dataset Dynamics via Gradient Flows in Probability Space

## 摘要

各种机器学习任务，从生成式建模到域适应，都围绕着数据集转换和操作的概念展开。 虽然存在各种用于转换 unlabeled 数据集的方法，但缺少同样目的的用于 labeled （例如，分类）数据集的原则性方法。 在这项工作中，我们提出了一种新的数据集转换框架，我们将其作为对数据生成联合概率分布的优化。 我们通过概率空间中的 Wasserstein 梯度流来解决这类问题，并为灵活但表现良好的一类目标函数推导出实用且有效的 particle-based 的方法。通过各种实验，我们表明该框架可用于对分类数据集施加约束，使其适应迁移学习，或重新利用固定或黑盒模型以高精度分类 previously unseen 的数据集。

# 1 介绍

机器学习实践的标志之一是与丰富的通用数据相比，特定领域和特定任务的数据相对稀缺。 因此，机器学习中的许多问题都涉及数据集操作或转换，以重新加权、sub-sample、增强、压缩或构建数据的生成模型。

最近关于这些问题的工作集中在 unlabeled 数据集上，要么只考虑无监督学习设置，要么忽略有监督学习中的label。 labeled 数据集的操作虽然探索较少，但因其各种潜在应用而具有吸引力。 一方面，it can unify, formalize, and cast under a new light various data manipulation heuristics core to state-of-the-art supervised learning pipelines (此句实在不知该如何翻译)，例如数据增强、数据集池化或实例混合（Zhang et al  ., 2018)。 另一方面——也是我们在这项工作中的主要动机——是这样一个框架解决缺少探索的或全新的问题的潜力，例如通过施加某些约束 (例如几何或隐私)来“塑造”数据集，或者将它们来从一个领域转换另一个领域，以此将迁移学习转化为：将数据迁移到某个模型的专业领域，而不是反着来。 当前的数据转换方法不适合这些任务，因为它们要么专门对特征进行操作，要么谨慎地、启发式地使用 label 。 此外，它们中的大多数只适用于特定问题，不容易泛化。 因此，缺少用于转换和操作数据集的统一框架。

针对这一点，在这项工作中，我们提出了一种原则性的、灵活的且计算上可行的 labeled 数据集转换方法。 基于上述 motivating 的应用，我们寻求一个**独立于模型的框架，仅依赖于数据的内在几何属性，并且适用于任何分类数据集，无论大小、维度或类别数量如何**。
我们实现这一目标的第一步是将数据集视为来自联合分布的样本，并专注于操作这些分布。 事实上，虽然数据集可能包含有限数量的样本，但精确的数量通常无关并且通常未指定（例如，在流设置中）。相反，真正感兴趣的是产生数据集的生成过程。 因此，我们将数据集表征为来自（未知）联合概率分布 $\rho(x, y)$ 的样本集合，并将其转换为概率空间 ${\cal P}({\cal X} × {\cal Y})$ 中的优化问题。 形式上，对于其分布编码了感兴趣的变换的一些函数 $F$  ,我们寻求解决形式为 $\min_{ \rho \in {\cal P}({\cal X} × {\cal Y}) }F(\rho)$ 的问题。

在这种无限维、非欧几里得的概率空间上形式化和解决优化问题在概念上是具有挑战性的。 在这项工作中，我们通过梯度流来实现，梯度流是在非常普遍的无限维空间中建模动力学的应用数学的关键（Ambrosio 等，2005），其实际上已经在度量测量空间背景下进行了广泛的研究 （Santambrogio，2017），并与偏微分方程（PDE）（Jordan 等，1998）有深层联系。 梯度流具有很多吸引人的特性：它们在自己能建模的潜在空间和动力学类型方面具有内在的灵活性，它们接受严格的收敛分析，并且除了最终的最小化解决方案之外，它们还产生了完整的迭代轨迹，这通常也很有用。 因此，梯度流最近成为分析和推导机器学习中（参数）优化方法的流行工具也就不足为奇了（Javanmard 等，2020 ；Chizat，2019 ）。

但是利用梯度流进行 labeled 数据集优化会带来各种挑战。 首先，必须定义 feature-label 对的合适表示以及数据集之间的有意义的度量。 然后，必须找到一类足够 expressive 的泛函 (functional)，可以在数据集上对感兴趣的目标函数进行建模，同时又表现得足够好以享有理论保证，且允许进行易处理的优化。 为了应对这些挑战中的第一个，我们利用最近提出的基于最佳传输 (optimal transport，OT) 的数据集间的距离的概念 (Alvarez-Melis & Fusi, 2020)。 从概念上讲，这个概念赋予联合（feature-label）分布空间一个度量，允许我们在上述度量测量空间的更一般设置中在这个空间中cast flows。 在实际层面上，我们可以使用这个距离作为函数来定义最小化它的梯度流，即：以编码给定数据集的相似性为优化目标。 这样做需要推广这个距离，它以前只用于静态数据集比较，并使其可微，产生一个我们可以用自动微分进行有效优化的损失函数。

为了解决第二个挑战——定义合适的函数目标——我们借用了一系列灵活的函数，这些函数在梯度流文献中得到了充分研究，展示了如何 re-purpose 它们来编码各种方便的与数据集相关的目标，并详细讨论如何在依赖于自动微分的梯度流的 particle-based 的近似中使用它们。 结果我们最终得到了一个灵活、高效且具有扎实理论基础的 labeled 数据集优化框架。

在我们的实验中，我们首先展示了这个数据集转换框架如何产生处理迁移学习问题的新方法。 例如，它可以与传统的模型自适应结合使用，作为 fine-tune 之前的数据预处理手段（第 7.2 节），但也可以代替它，用于模型自适应不可行的设置（例如，frozen、黑盒或超大模型）。对于此类具有挑战性的设置，我们基于流的方法允许我们”re-purpose“已经训练过的模型，对 previously unseen 的数据集进行高精度分类（第 7.3 节）。 我们还展示了该框架可用于生成具有各种几何约束的数据集（第 7.1 节），展示其作为数据集综合 (dataset synthesis) 的原则性工具的前景。

## 2 相关工作

**Dataset adaptation with optimal transport**：之前的大多数使用 OT (optimal transport) 来比较数据集的工作，专门针对 feature 而非 label ，或者依赖于分类损失，有的还假设两个 label 集合是相同的。 

## 3 技术背景

我们首先介绍我们框架背后的两个关键概念：最优传输和梯度流

### 3.1 设置和标识

设 $\cal X$ 是配备度量 $d$ 的 Polish 空间，${\cal P}({\cal X} ) $是该空间上具有有限二阶矩的非负 Borel measures的集合。 我们考虑连续和离散度量，后者表示为经验分布

${\cal T}_\beta (\rho)=\rm OTDD(\rho,\beta)$​

