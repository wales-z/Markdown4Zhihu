# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 摘要

我们引入了一种称为 BERT 的新语言表示模型，它代表来自 Transformers 的双向编码器表示 (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)。与最近的语言表示模型（Peters 等人，2018a；Rad Ford 等人，2018 ）不同，BERT 旨在通过对所有层的左右上下文进行联合调节，从无标签的文本中预训练深度双向表示。因此，预训练的 BERT 模型可以仅通过一个额外的输出层进行微调，从而为各种任务（例如问答和语言推理）创建最先进的模型，而无需大量 task-specific 的架构修改。

BERT 在概念上很简单，在经验上也很强大。它在 11 项自然语言处理任务上获得了最新的最新结果，包括将 GLUE 分数提高到 80.5%（提高 7.7%），MultiNLI 准确率提高到 86.7%（提高 4.6%），SQuAD v1.1 问答的测试 F1 达到 93.2（提高 1.5 point）和 SQuAD v2.0 测试 F1 达到 83.1（绝对提高 5.1 point）。

## 1 引言

语言模型预训练已被证明可有效改进许多自然语言处理任务（Dai 和 Le，2015；Peters 等，2018a；Radford 等，2018；Howard 和 Ruder，2018）。这些包括句子级任务，例如自然语言推理（Bowman 等人，2015 ；Williams 等人，2018 ）和释义 (paraphrasing)（Dolan 和 Brockett，2005 ），旨在通过整体分析来预测句子之间的关系，以及 token 级别的任务，例如命名实体识别和问答，其中模型需要在 token 级别产生细粒度的输出（Tjong Kim Sang 和 De Meulder，2003 ；Rajpurkar 等人，2016 ）。

将预训练语言表示应用于下游任务有两种现有策略：基于特征和微调。基于特征的方法，例如 ELMo（Peters 等人，2018a），使用 task-specific 的架构，其中包括预训练的表示作为附加特征。微调方法，例如生成式预训练 Transformer (OpenAI GPT) (Radford et al., 2018)，引入了最少的 task-specifc 的参数，并通过简单地微调所有预训练的任务来训练下游任务参数。这两种方法在预训练期间共享相同的目标函数，它们使用单向语言模型来学习通用语言表示

我们认为当前的技术限制了预训练表示的能力，尤其是微调方法。主要限制是标准语言模型是单向的，这限制了可以在预训练期间使用的架构的选择。例如，在 OpenAI GPT 中，作者使用从左到右的体系结构，其中每个 token 只能趋向于 Transformer 的自注意力层中的先前 token（Vaswani 等，2017）。这种限制对于句子级任务来说是次优的，并且在将基于微调的方法应用于诸如问答之类的 token 级任务 (这些任务中，从两个方向合并上下文至关重要) 时可能有很大坏处。

在本文中，我们通过提出 BERT：来自 Transformers 的双向编码器表示，来改进基于微调的方法。
受完形填空任务（Taylor，1953）的启发，BERT 通过使用“掩码语言模型”（mask language model,  MLM）预训练目标减轻了前面提到的单向性约束。屏蔽语言模型从输入中随机屏蔽一些 tokens，目标是仅根据其上下文来预测屏蔽词的原始词汇的 id。 与从左到右的语言模型预训练不同，MLM 目标使得获得的表示能够融合左右上下文，这使我们能够预训练深度双向 Transformer。除了掩码语言模型，我们还使用“下一句预测 (next sentence prediction) ”任务，联合预训练 text-pair 表示。 我们这篇论文的贡献如下：

- 我们证明了双向预训练对语言表示的重要性。不像拉德福德等人 (2018) 使用单向语言模型进行预训练，BERT 使用掩码语言模型来生成预训练的深度双向表示。这也与 Peters 等人的观点相反 (2018a)，它使用独立训练的从左到右和从右到左 LM 的浅层级联。
- 我们表明，预训练的表示减少了对许多精心设计的任务特定架构的需求。  BERT 是第一个基于微调的表示模型，它在大量句子级和 token 级任务上实现了最先进的性能，优于形成许多特定于任务的架构。
- BERT 提高了十一项 NLP 任务的最新技术水平。代码和预训练模型可从 https://github.com/google-research/bert 获得。

## 2 相关工作

预训练通用语言表示的历史悠久，我们在本节中简要回顾了最广泛使用的方法。

### 2.1 无监督的基于特征的方法

几十来，学习广泛适用的单词表示一直是一个活跃的研究领域，包括非神经的（Brown 等，1992；Ando 和 Zhang，2005；Blitzer 等，2006）和神经的（Mikolov 等，2013） ; Pennington et al., 2014) 方法。 预训练的词嵌入是现代 NLP 系统的一个组成部分，它比从头学习的嵌入提供了显着的改进（Turian 等，2010）。 为了预训练词嵌入向量，前人使用了从左到右的语言建模目标（Mnih 和 Hinton，2009），以及在左右上下文中区分正确单词和错误单词的目标（Mikolov 等人，2013)。

这些方法已被推广到更粗的粒度，例如句子嵌入（Kiros 等人，2015；Logeswaran 和 Lee，2018）或段落嵌入（Le 和 Mikolov，2014 年）。为了训练句子表示，先前的工作使用目标函数对候选的下一个句子进行排名（Jernite 等人，2017 年；Logeswaran 和 Lee，2018 年），给定前一个句子的表示，从左到右生成下一个句子单词（  Kiros 等人，2015 年），或去噪自动编码器衍生目标（Hill 等人，2016 年）。

ELMo 及其前身 (Peters et al., 2017, 2018a) 沿不同维度概括了传统的词嵌入搜索。 他们从从左到右和从右到左的语言模型中提取上下文敏感的特征。每个标记的上下文表示是从左到右和从右到左表示的串联。在将上下文词嵌入与现有的特定任务架构集成时，ELMo 提升了几个主要 NLP 基准（Peters 等人，2018a）的最新技术水平，包括问题回答（Rajpurkar 等人，2016）、情感分析（Socher 等人，2013）和命名实体识别（Tjong Kim Sang 和 De Meulder，2003）。 梅拉姆德等人 (2016) 提出通过使用 LSTM 从左右上下文预测单个单词的任务来学习上下文表示。与 ELMo 类似，他们的模型是基于特征的，而不是深度双向的。 费杜斯等人 (2018) 表明完形填空任务可用于提高文本生成模型的鲁棒性。

### 2.2 无监督的微调方法

与基于特征的方法一样，第一个在这个方向上的工作仅从无标签的文本中预训练词嵌入参数（Col lobert 和 Weston，2008）。

最近，产生上下文标记表示的句子或文档编码器已从无标签的文本中进行预训练，并针对有监督的下游任务进行了微调（Dai 和 Le，2015 年；Howard 和 Ruder，2018 年；Radford 等人，2018 年）。这些方法的优点是需要从头开始学习的参数很少。至少部分由于这一优势，OpenAI GPT（Radford 等人，2018 年）在 GLUE 基准测试（Wang 等人，2018a）的许多句子级任务上取得了先前的最新成果。从左到右的语言建模和自动编码器目标已用于预训练此类模型（Howard 和 Ruder，2018 年；Radford 等人，2018 年；Dai 和 Le，2015 年）

### 2.3 源自有监督数据的迁移学习

也有工作显示，从具有大型数据集的监督任务中进行有效转移，例如自然语言推理（Conneau 等人，2017 年）和机器翻译（McCann 等人，2017 年）。计算机视觉研究也证明了从大型预训练模型迁移学习的重要性，其中一个有效的方法是微调使用 ImageNet 预训练的模型（Deng 等人，2009 ；Yosinski 等人，2014)

## 3 BERT

我们将在本节中介绍 BERT 及其详细实现。我们的框架有两个步骤：预训练和微调。在预训练期间，模型在不同预训练任务上的未标记数据上进行训练。对于微调，BERT 模型首先使用预训练的参数进行初始化，然后使用来自下游任务的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是使用相同的预训练参数初始化的。图 1 中的问答示例将作为本节的运行示例。

![image-20211106144739331](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211106144739331.png)

【图 1】

BERT 的一个显着特点是其跨不同任务的统一架构。预训练架构和最终下游架构之间存在最小差异。

#### 模型架构 

BERT 的模型架构是一种基于 Vaswani 等人描述的原始实现的多层双向 Transformer 编码器。  (2017) 并在 tensor2tensor 库中发布^1^。由于 Transformer 的使用已经变得普遍，而且我们的实现几乎与原始实现相同，因此我们将省略对模型架构的详尽背景描述，并请读者参考 Vaswani 等(2017) 以及优秀的指南，例如“The Annotated Transformer”^2^。

在这项工作中，我们将层数（即 Transformer 块）表示为 $L$，将隐藏大小 (hidden size) 表示为 $H$，将自注意力头的数量表示为 $A$^3^ 我们主要报告两种模型大小的结果：${\rm BERT_{BASE}} (L=12, H=768, A=12, \rm Total \ Parameters=110M) $和 ${\rm BERT_{LARGE}} (L=24, H=1024, A=16, \rm Total \ Parameters=340M )$。
$\rm BERT_{BASE }$被选为与 OpenAI GPT 具有相同的模型大小以进行比较。然而，至关重要的是，BERT Transformer 使用双向自注意力，而 GPT Transformer 使用受约束的自注意力，其中每个 token 只能注意其左侧的上下文^4^。

> 注释 1：https://github.com/tensorflow/tensor2tensor
>
> 注释 2：http://nlp.seas.harvard.edu/2018/04/03/attention.html
>
> 注释 3：在所有情况下，我们将前馈/滤波器大小设置为 $4H$，即 $H = 768$ 时为 $3072$，$H = 1024$ 为 $4096$。
>
> 注释4：我们注意到，在文献中，双向 Transformer 通常被称为“Transformer 编码器”，而仅左上下文版本被称为“Transformer 解码器”，因为它可以用于文本生成。

#### 输入/输出表示

为了让 BERT 处理各种下游任务，我们的输入表示能够在一个标记序列中明确表示单个句子和一对句子（例如，h Question、Answeri）。

在整个工作中，“句子”可以是连续文本的任意跨度，而不是实际的语言句子。 “序列”是指输入到 BERT 的 token 序列，可以是单个句子，也可以是两个打包在一起的句子。

我们使用 WordPiece 嵌入（Wu 等人，2016 ）和 30,000 个 token 的词汇表。每个序列的第一个标记始终是一个特殊的分类标记 ($\rm [CLS]$)。与此标记对应的最终隐藏状态用作分类任务的聚合序列表示。句子对被打包成一个序列。 我们以两种方式区分句子。首先，我们用一个特殊的标记 ([SEP]) 将它们分开。其次，我们向每个标记添加一个学习嵌入，指示它属于句子 A 还是句子 B。 如图 1 所示，我们将输入嵌入表示为 $E$，将特殊 $\rm [CLS]$ 标记的最终隐藏向量表示为 $C \in  \mathbb R^H$，以及第 $i$ 个输入 token 的最终隐藏向量为 $T_i \in \mathbb  R^H$。

对于给定的标记，其输入表示是通过对相应的 token、segment 和 position 嵌入求和来构建的。这种结构的可视化可以在图 2 中看到。

![image-20211106153820317](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211106153820317.png)

### 3.1 预训练 BERT

与Peters等人(2018a) 和 Radford 等人(2018)不同，我们不使用传统的从左到右或从右到左的语言模型来预训练 BERT。相反，我们使用本节中描述的两个无监督任务对 BERT 进行预训练。此步骤显示在图 1 的左侧部分。

#### 任务 #1：Masked LM

直觉上，我们有理由相信深度双向模型比从左到右模型或从左到右和从右到左模型的浅层级联更强大。不幸的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件会允许每个词直接“看到自己”，并且模型可以简单地在一个多层次的上下文中预测目标词。

为了训练深度双向表示，我们简单地随机屏蔽一定比例的输入 token，然后预测那些被屏蔽的标记。我们将此过程称为“掩码 LM”（MLM），尽管它在文献中经常被称为完形填空任务（Taylor，1953）。在这种情况下，与掩码标记相对应的最终隐藏向量被馈送到词汇表上的输出 softmax，就像在标准 LM 中一样。在我们所有的实验中，我们随机将所有 WordPiece 的 15% 屏蔽为每个序列中的 kens。与去噪自动编码器 (Vincent et al., 2008) 相比，我们只预测被屏蔽的词，而不是重建整个输入。

虽然这允许我们获得双向预训练模型，但缺点是我们在预训练和微调之间造成了不匹配，因为 $\rm [MASK]$ token 在微调期间不会出现。为了缓解这种情况，我们并不总是用实际的 [MASK] 标记替换“屏蔽”词。训练数据生成器随机选择 15% 的 token 位置进行预测。如果选择了第 i 个 token ，我们将第 $i$ 个 token 替换为 (1) 80% ， $\rm [MASK] $ token  (2) 10% ，随机 token  (3) 10%，未更改的第 $i$ 个 token 。然后，$T_i$ 将用于预测具有交叉熵损失的原始 token 。我们在附录 C.2 中比较了此程序的变体。

#### 任务 #2：下一句子预测（NSP）

许多重要的下游任务，例如问答 (QA) 和自然语言推理 (NLI)，都是基于理解两个句子之间的关系，而这不是语言建模直接捕获的。为了训练一个能理解句子关系的模型，我们对二值化的下一个句子预测任务进行了预训练，该任务可以从任何单语语料库中轻松生成。具体来说，当为每个预训练示例选择句子 A 和 B 时，50% 的时间 B 是 A 之后的实际下一个句子（标记为 $\rm IsNext$），50% 的时间是来自语料库的随机句子 （标记为 $\rm NotNext$）。 如图 1 所示，$C$ 用于下一句预测 (NSP)^5^。尽管它很简单，但我们在 5.1 节中证明，针对此任务的预训练对 QA 和 NLI 都非常有益^6^。

>注释 5：最终模型在 NSP 上达到了 97%-98% 的准确率。
>
>注释 6：向量 $C$ 没有经过微调就不是有意义的句子表示，因为它是用 NSP 训练的。

NSP 任务与 Jernite 等人 (2017) 以及 Logeswaran 和 Lee (2018) 使用的表示学习目标密切相关。然而，在之前的工作中，只有句子嵌入被转移到下游任务，而 BERT 转移所有参数来初始化末端任务模型参数。

#### 预训练数据

预训练过程很大程度上遵循了关于语言模型预训练的现有文献。对于预训练语料库，我们使用 BooksCorpus（800M 词）（Zhu et al., 2015）和英文维基百科（2,500M 词）。对于维基百科，我们只提取文本段落而忽略列表、表格和标题。使用文档级语料库而不是像 Billion Word Benchmark (Chelba et al., 2013) 这样的打乱的句子级语料库来提取长的连续序列是至关重要的

### 3.2 微调 BERT

微调很简单，因为 Transformer 中的自注意力机制允许 BERT 通过交换适当的输入和输出来模拟许多下游任务——无论它们涉及单个文本还是文本对。对于涉及文本对的应用程序，一个常见的模式是在应用双向交叉注意力之前对文本对进行独立编码，例如 Parikh 等人 (2016); 徐等人 (2017)。相反，BERT 使用自注意力机制来统一这两个阶段，因为编码具有自注意力的连接文本对有效地包括两个句子之间的双向交叉注意力

对于每个任务，我们只需将任务特定的输入和输出插入到 BERT 中，并端到端地微调所有参数。在输入端，预训练中的句子 A 和句子 B 类似于 (1) 释义中的句子对，(2) entailment 中的假设-前提对，(3) 问答中的问题-段落对，以及 (4) 文本分类或序列标注中的 degenerate $text-\empty$ 对。在输出端，token 表示被送入一个token级别任务的输出层，例如序列标记或问答，而 $\rm [CLS]$ 表示被送入一个输出层进行分类，例如entailment或情感分析。

与预训练相比，微调相对便宜。 从完全相同的预训练模型开始，论文中的所有结果的复现最多需要在单个 Cloud TPU 上 运行1 小时，或在 GPU 上运行几个小时^7^ 。我们第 4 节的相应小节中的描述了 task-specific 的具体细节。更多细节可以在附录 A.5 中找到。

> 注释 7：例如，BERT SQuAD 模型可以在单个 Cloud TPU 上训练大约 30 分钟，以达到 91.0% 的 Dev F1 分数。



## 附录

### A  BERT的附加细节

#### A.1  预训练任务的阐述

我们在下面提供了预训练任务的示例。

##### Masked LM 和 Mask 过程

假设一个无标签的句子是 my dog is hairy，在随机掩蔽过程中我们选择了第 4 个标记（对应于 hairy），我们的 mask 过程可以进一步说明为

- 80% 的时间：用 [MASK] 标记替换单词，例如，my dog is hairy → my dog is [MASK]
- 10% 的时间：用随机词替换单词，例如，my dot is hairy → my dot is apple
- 10% 的时间：保持单词不变，例如，my dot is hairy → my dot is hairy . 这样做的目的是使输出的表示偏向于实际观察到的单词。

这个过程的优点是 Transformer 编码器不知道它将被要求预测哪些词或哪些词已被随机词替换，因此它被迫保留每个输入 token 的分布式上下文表示。此外，因为随机替换只发生在所有标记的 1.5%（即 15% 中的 10%），这似乎不会损害模型的语言理解能力。在第 C.2 节中，我们评估了此过程的影响。

与标准的语言模型训练相比，masked LM 仅对每个 batch 中 15% 的 token 进行预测，这表明模型可能需要更多的预训练步骤才能收敛。在第 C.1 节中，我们证明了 MLM 确实比从左到右的模型（预测每个标记）收敛速度稍慢，但是 MLM 模型的提升远比增加的训练成本更值得。

##### 下一句子预测

下面的例子可以说明下一句预测任务。

**输入**：[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]

**Label**：IsNext

**输入**：[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]

**Label**：NotNext

#### A.2 预训练过程

为了生成每个训练输入序列，我们从语料库中采样了两段文本，我们将其称为“句子”，尽管它们通常比单个句子长得多（但也可以更短）。第一个句子接收 A 的嵌入，第二个句子接收 B 嵌入。50% 的时间 B 是 A 之后的实际的下一个句子，50% 的时间它是一个随机句子，这是为“下一个句子预测”任务完成的。它们被采样，使得组合长度≤ 512 个 token。LM masking 是在 WordPiece token 化之后应用的，统一掩码率为 15%，对部分词块没有特殊考虑。

我们以 256 个序列的batch size（256 个序列 * 512 个token = 128,000 个tokens/batch）训练 1,000,000 个 step，在 33 亿个单词语料库中大约 40 个 epoch。我们使用 Adam，学习率为 1e-4，$\beta_1 = 0.9, \beta_2 = 0.999$，L2 weight decay 为 0.01，前 10,000 步进行学习率预热，学习率线性衰减。我们在所有层上使用 0.1 的 dropout probability。遵循 OpenAI GPT，我们使用 gelu 激活（Hendrycks 和 Gimpel，2016）而不是标准 relu。训练损失是平均的masked LM 似然和平均的下一句子预测似然之和。

$\rm BERT_{BASE }$的训练在 Pod 配置中的 4 个 Cloud TPU 上进行(总共 16 个 TPU 芯片)^13^。$\rm BERT_{LARGE}$ 的训练在 16 个 Cloud TPU 上进行（总共 64 个 TPU 芯片）。每次预训练需要 4 天时间才能完成。

更长的序列会不成比例地 (disproportionately) 昂贵，因为注意力是序列长度的二次方。为了在我们的实验中加速预训练，我们对模型的 90% 的步骤进行了序列长度为 128 的预训练。然后，我们训练 512 序列的其余 10% 的步骤来学习位置嵌入。

#### A.3 微调步骤

对于微调，大多数模型超参数与预训练中的相同，但 batch size、学习率和训练 epoch 数除外。dropout probability 始终保持在 0.1。 最佳超参数值是 task-specifc 的，但我们发现以下可能值范围适用于所有任务：

- Batch size：16, 32
- 学习率（Adam）：5e-5. 3e-5, 2e-5
- epoch数量：2, 3, 4

我们还观察到，与小数据集相比，大型数据集（例如 10 万多个带标签的训练示例）对超参数选择的敏感度要低得多。微调通常非常快，因此只需对上述参数进行彻底搜索并选择在开发集上表现最佳的模型是合理的。

#### A.4 BERT, ELMo, OpenAI GPT的比较

在这里，我们研究了最近流行的表示学习模型（包括 ELMo、OpenAI GPT 和 BERT）的差异。 模型架构之间的比较如图 3 所示。请注意，除了架构差异之外，BERT 和 OpenAI GPT 是微调方法，而 ELMo 是基于特征的方法。

![image-20211106164737287](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211106164737287.png)

与 BERT 最具可比性的现有预训练方法是 OpenAI GPT，它在大型文本语料库上训练从左到右的 Transformer LM。事实上，BERT 中的许多设计都是有意使其尽可能接近 GPT，以便将两种方法进行最小程度的比较。 这项工作的核心论点是，3.1 节中介绍的**双向性和两个预训练任务占了大部分经验提升**，但我们确实注意到 BERT 和 GPT 的训练方式之间还有其他一些差异

- GPT 是在 BooksCorpus 上训练的（800M 词）；BERT 在 BooksCor pus（800M 词）和维基百科（2500M 词字）上接受过训练。
- GPT 使用仅在微调时引入的句子分隔符（[SEP]）和分类器标记（[CLS]）；BERT 在预训练期间学习 [SEP]、[CLS] 和句子 A/B 嵌入。
- GPT 训练了 100 万步，batch size为 32,000 词；  BERT 训练了 100 万步，batch size 为 128,000 字。
- GPT 在所有微调实验中使用相同的 5e-5 学习率；  BERT 选择 task-specific 的微调学习率，该学习率应在开发集上表现最佳。

为了隔离这些差异的影响，我们在第 5.1 节中进行了消融实验，这表明大部分改进实际上来自两个预训练任务及其实现的双向性

#### A.5 不同任务中的微调的阐述

在不同任务上微调 BERT 的图示可以在图 4 中看到。我们的 task-specific 模型是通过将 BERT 与一个额外的输出层相结合而形成的，因此只需要最少量的从头开始学习的参数。在这些任务中，（a）和（b）是序列级任务，而（c）和（d）是 token 级任务。 图中，E表示输入嵌入，$T_i$表示 token i 的上下文表示，[CLS] 是分类输出的特殊符号，[SEP] 是分离非连续 token 序列的特殊符号。



![image-20211106164909530](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211106164909530.png)

