# Neural Natural Language Inference Models Enhanced with External Knowledge

## 摘要

对自然语言推理建模是一项非常具有挑战性的任务。 随着大量注释数据的可用性，最近在推理模型中训练复杂模型（例如基于神经网络的推理模型）变得可行，这些模型已显示出达到最先进的性能。尽管有着大量的标注数据，但机器能否从这些数据中学习执行自然语言推理 (NLI) 所需的所有知识？ 如果不是，基于神经网络的 NLI 模型如何从外部知识中受益，以及如何构建 NLI 模型来利用它？在本文中，我们用外部知识丰富了最先进的神经自然语言推理模型。 我们证明了所提出的模型改进了神经 NLI 模型，以在 SNLI 和 MultiNLI 数据集上实现最先进的性能。

## 1 引言

推理 (reasoning and inference) 是人类和人工智能的核心。 自然语言推理 (NLI)，也称为文本蕴涵识别 (RTE)，是一个重要的 NLP 问题，涉及确定前提 $p$ 和 假设 $h$。 一般来说，在语言中建模非正式推理是实现真正的自然语言理解的一个非常具有挑战性和基本的问题。

在过去几年中，更大的注释数据集被提出，例如 SNLI和 MultiNLI 数据集，这使得训练相当复杂的基于神经网络的、包含大量参数的模型建模变得可行。 此类模型已证明可实现最先进的性能。

虽然神经网络已被证明在使用大量训练数据对 NLI 建模时非常有效，但它们通常通过假设所有推理知识都可以从提供的训练数据中学习来专注于端到端训练。 在本文中，我们放宽了这一假设，并探讨了外部知识是否可以进一步帮助 NLI。考虑一个例子：

- $p$: A lady standing in a wheat field.
- $h$: A person standing in a corn field

在这个简化的例子中，当计算机被要求预测这两个句子之间的关系时，如果训练数据没有提供“小麦”和“玉米”之间关系的知识（例如，如果两个词之一没有出现在训练数据中或者它们没有在任何前提-假设对中配对），计算机将很难正确识别前提与假设相矛盾。

总的来说，尽管在许多任务中学习 tabula rasa 实现了最先进的性能，但我们相信复杂的 NLP 问题（如 NLI）可以从利用人类积累的知识中受益，特别是在可预见的未来，当机器不能靠自己学习的时候。

在本文中，我们用共同注意力 (co-attention)、局部推理收集和推理组合组件方面的外部知识丰富了基于神经网络的 NLI 模型。 我们展示了所提出的模型改进了最先进的 NLI 模型，以在 SNLI 和 MultiNLI 数据集上获得更好的性能。 当训练数据规模受到限制时，利用外部知识的优势更加显着，表明如果能够获得更多的知识，可能会带来更多的好处。 除了获得最先进的性能外，我们还想了解外部知识如何为基于神经网络的典型 NLI 模型的主要组成部分做出贡献。



## 3 具有外部知识的基于神经网络的 NLI 模型

在本节中，我们提出了基于神经网络的 NLI 模型来合并外部推理知识，正如我们稍后将在第 5 节中展示的那样，实现了最先进的性能。除了获得领先的性能外，我们还对研究外部知识对基于神经网络的 NLI 建模主要组件的影响很感兴趣。

图 1 显示了所提议框架的高级概览。 虽然具体的 NLI 系统在其实现上有所不同，但典型的最先进 NLI 模型包含表示前提和假设句子、收集局部推理信息（例如词汇）和聚合并组合局部信息以在句子级别做出全局决策。 我们在这些主要的 NLI 组件中相应地合并和调查外部知识：计算共同注意力，收集局部推理信息，并组合推理以做出最终决定。

![image-20210912165205180](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20210912165205180.png)

【图1】

### 3.1 外部知识

如上所述，虽然 NLI 存在大量的标注数据，但机器能否从数据中学习执行 NLI 所需的所有推理知识？ 如果不是，基于神经网络的 NLI 模型如何从外部知识中受益，以及如何构建 NLI 模型来利用它？

我们研究将外部推理相关知识纳入神经网络的主要组件以进行自然语言推理。 例如，直观地了解给定单词之间的同义词、反义词、上位词和下位词可能有助于建模前提和假设之间的软对齐； 有关上位词和下位词的知识可能有助于捕捉蕴涵； 关于反义词和共下位词（共享同一个上位词的词）的知识可能有益于矛盾建模。

在本节中，我们讨论将基本的、词汇级别的语义知识纳入神经 NLI 组件。 具体来说，我们考虑词 $w_i$ 和 $w_j$ 之间的外部的、词汇级别的推理知识，它表示为向量 $r_{ij}$ 并被合并到图 1 所示的三个特定组件中。我们稍后在实验设置部分（第 4 节）中讨论 $r_{ij}$ 是如何构建的，但重点关注本节中提出的模型。 请注意，虽然我们在论文中研究了词汇级别的推理知识，但如果有关更大块的文本对（例如，短语之间的推理关系）的推理知识可用，则可以轻松扩展所提出的模型来处理该问题。 在本文中，我们改为让 NLI 模型组合词汇级别的知识，以获得较大文本之间的推理关系。

### 3.2 编码前提和假设

与之前的许多工作相同，我们使用双向 LSTM（BiLSTM）对前提和假设进行编码。 前提表示为 $\boldsymbol a = (a_1, . . , a_m)$，假设为 $\boldsymbol b = (b_1, . . , b_n)$，其中 $m$ 和 $n$ 是句子的长度。 然后将 $a$ 和 $b$ 使用embedding 矩阵 $\bold E\in {\mathbb R^{d_e \times |V|}}$ 嵌入到 $d_e$ 维向量 $[\bold E(a_1), .  .  .  , \bold E(a_m)]$ 和 $[\bold E(b_1), .  .  .  ,\bold E(b_n)]$  ，其中 $|V|$ 是词汇量大小，$\bold E$ 可以用预训练的词 embedding 进行初始化。 为了在上下文中表示单词，前提和假设被输入 BiLSTM 编码器以获得依赖于上下文的隐藏状态 $\boldsymbol a^s$ 和 $\boldsymbol b^s$ ：
$$
\boldsymbol a_i^s= {\rm Encoder}(\bold E(\boldsymbol a),i)
\tag1
$$

$$
\boldsymbol b_j^s= {\rm Encoder}(\bold E(\boldsymbol b),j)
\tag2
$$

其中 $i$ 和 $j$ 分别表示前提的第 $i$ 个词和假设的第 $j$ 个词。



### 3.3 Knowledge-Enriched 共同注意力

如上所述，前提和假设之间单词对的软对齐可以受益于Knowledge-Enriched共同注意力机制。 给定前提的第 $i$ 个词和假设的第 $j$ 个词之间的关系特征 $\boldsymbol r_{ij} \in {\mathbb R}^{d_r}$ 从外部知识推导出来，共同注意力计算如下：
$$
e_{ij} = (\boldsymbol a_i^s)^T\boldsymbol b_j^s + F(\boldsymbol r_{ij})
\tag3
$$
函数 $F$ 可以是任何非线性或线性函数。 在本文中，我们使用 $F(\boldsymbol r_{ij}) = λ\mathbb 1(\boldsymbol r_{ij})$，其中 λ 是在开发集上调整的超参数，${\mathbb 1}$ 是指示函数，如下所示
$$
{\mathbb 1}(\boldsymbol r_{ij})=
\begin{cases}
1 \ \ \ ,当 \boldsymbol r_{ij} 为非0向量\\
0 \ \ \ ,当 \boldsymbol r_{ij} 为0向量
\end{cases}
\tag4
$$
直观地，具有语义关系的词对，例如同义词、反义词、上位词、下位词和共同下位词，可能对齐在一起。
我们将在第 4 节讨论如何构建外部知识。我们还尝试了两层 MLP 作为函数 $F$ 中的通用函数拟合器来学习底层组合函数，但在开发数据集上没有观察到比我们获得的最佳性能有进一步的改进。

软对齐由等式（3）中计算的共同注意力矩阵 $\bold e \in \mathbb R^{m\times n}$ 决定，用于获得前提和假设之间的局部相关性。 对于前提中第 $i$ 个单词的隐藏状态，即 $\boldsymbol a_i^s$（已经对单词本身及其上下文进行了编码），使用 $e_{ij}$ 将假设中的相关语义识别为上下文向量 $\boldsymbol a_i^c$，更具体地说，如等式 (5)：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20210912170447095.png" alt="image-20210912170447095" style="zoom:80%;" />

其中 $\alpha \in \mathbb R^{m \times n}$ 和  $\beta \in \mathbb R^{m \times n}$ 是关于2-aixs和1-asix的归一化注意力权重矩阵。对假设中的每个词进行相同的计算，即 $\boldsymbol b_j^s$ ，用等式（6）得到上下文向量 $\boldsymbol b_j^c$

### 3.4 使用外部知识的局部推理收集

通过比较  $\boldsymbol a_i^s$（前提中一个单词的表示）和 $\boldsymbol a_i^c$（来自假设的、与单词 $\boldsymbol a_i^s$ 对齐的上下文表示）之间的推理相关语义关系，我们在对齐的词对之间建模局部推理（即单词级别的推理）。 直观地说，例如，有关上位词或下位词的知识可能有助于建模蕴涵 (entailment) ，而有关反义词和共下位词的知识可能有助于建模矛盾 (contadiction)。 通过比较 $\boldsymbol a_i^s$ 和 $\boldsymbol a_i^c$ ，除了它们与外部知识的关系之外，我们还可以获得每个词的单词级别的推理信息。 对 $\boldsymbol b_j^s$ 和 $\boldsymbol b_j^c$ 执行相同的计算。 因此，我们收集到了 knowledge-enriched 的局部推理信息

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20210912172440891.png" alt="image-20210912172440891" style="zoom:80%;" />

其中使用了具有差异和元素乘积的启发式匹配技巧。 等式 (7) (8)中 的最后一项用于从外部知识中获取词级推理信息。 以式(7)为例，rij是前提中第i个词与假设中第j个词的关系特征，但我们更关心前提和假设中对齐词对之间的语义关系 假设。 因此，我们通过软对齐权重 αij 使用软对齐版本。 对于前提中的第i个词，等式（7）中的最后一项是基于第i个词与对齐词之间的外部知识的词级推理信息。 在等式 (8) 中执行相同的假设计算。  G 是一个非线性映射函数，用于降低维数。
具体来说，我们使用带有ReLU激活函数的1层前馈神经网络和shortcut connection，即以输入Pn P j=1 αijrij（或mi=1 βijrji）作为输出连接ReLU之后的隐藏状态 ami（或 bmj ）。