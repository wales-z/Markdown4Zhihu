# Sentiment-Aware Deep Recommender System With Neural Attention Networks

## 摘要

随着网络技术的出现，用户生成的文本评论在许多电子商务网站上越来越多。 这些评论不仅包含用户对产品不同 aspect 的评论，还包含与这些 aspect 相关的用户情绪。 尽管这些用户情绪是提高推荐系统性能的重要辅助信息，但大多数现有方法都忽略了在对细粒度用户-物品交互进行建模以提高推荐系统性能时充分利用它们。 因此，本文提出了一种具有神经注意力网络（SDRA）的情感感知深度推荐系统，该系统可以捕获产品的各个 aspect 以及与这些 aspect 相关的潜在用户情绪，以提高推荐系统的性能。 特别是，半监督主题模型旨在从用户文本评论中提取产品的各个 aspect 和相关联的情感词典 (sentiment lexicons)，然后通过交互式神经注意力机制将其合并到长短期记忆 (LSTM) 编码器中，以便更好地学习用户和物品情绪感知表示。 此外，引入了共同注意力 (co-attention) 机制以更好地对细粒度的用户-物品交互进行建模，以提高预测性能。 在不同数据集上的大量实验表明，我们提出的 SDRA 模型可以比基线方法获得更好的性能。

## 1 引言

推荐系统旨在解决信息过载的问题，从而帮助客户从各种备选方案中获得最佳选择。 基本上，推荐系统可以使用多种方法来实现，例如协同过滤（CF）和基于内容的 [1]。CF 方法已被证明是推荐系统中使用最广泛的技术 [2]。 这些方法的基本思想是，过去具有相似消费习惯的用户在未来倾向于共享相似的物品。 大多数协同过滤方法通常基于矩阵分解 (MF) 方法 [3]，该方法特别使用潜在因子来计算用户对物品的未知评级。 尽管这些方法在许多应用中取得了显着的成功，但它们通常存在一些问题，包括数据稀疏性。

随着亚马逊和 Yelp 等电子商务网站的进步，如今，已经提出了几种方法 [4]-[6] 来利用自由文本 (free-text) 评论来提高推荐系统的性能。 其中许多方法使用主题建模 (topic modelling)  [4]、[7]、[8] 来自动提取 aspect 并将它们与潜在因素模型集成以进行评级预测。 现有基于主题模型的方法的主要缺点之一是它们无法捕获单词的上下文信息 [9]，这已被证明对推荐系统的有效性能至关重要。

随着最近表征学习的成功，已经引入了几种方法来利用深度学习来构建推荐系统 [1]、[10]-[14]。 这些方法中的大多数利用卷积神经网络 (CNN) 模型 [1]、[12]、[13] 对用户和物品评论进行联合建模，以提高预测性能。

尽管这些方法已被证明比最先进的评分预测方法更有效，但是，上述所有方法在对用户和物品表示进行建模时都特别忽略了充分考虑潜在用户情绪。 在实际情况中，用户不仅对特定产品的不同 aspect 发表评论，而且对这些 aspect 表达不同的情感极性。 例如，当用户为一家餐厅写评论时，他可能会写一些句子来表达他对餐厅的位置和服务的不满，以及一些句子来表达他对价格和菜肴的满意。 不幸的是，在对用户物品表示进行建模时，这些用户情绪通常被忽略。然而，用户情绪作为其偏好的关键指标，通常表达用户对某项产品的不满或满意程度。 本文建议在用户/物品表示学习中充分考虑这一关键信息，以获得更好的预测性能。

上述方法的另一个缺点是它们以静态和独立的方式专门对潜在特征向量进行建模。 通过这种方式，用户和物品被投影到共享空间中固定的低维表示向量中。 很直观的是，并非评论中的所有单词都同样重要并且与用户对特定产品的评级相关。 例如，评论中的某些词可能会解释电影的情节，而此类信息可能与用户的整体偏好无关。 这基本上是因为评论中的每个词通常都侧重于用户体验的一个独特 aspect ，例如笔记本电脑的价格、性能甚至电池的耐用性。通过同时考虑用户和物品评论来识别相关语义信息可能是提高推荐系统性能的新途径。

考虑到上述动机，本文提出了一种情感感知深度推荐系统，通过将主题模型纳入深度学习方法，使用神经注意力机制有效捕获产品的领域特定的 aspect 和相应的用户情感

该模型包括四个主要组件：（1）LSTM 编码器，旨在捕获单词的上下文和长依赖信息（2）半监督主题模型，用于提取领域特定的 aspect 和情感词典。  (3) 一种共同注意机制，可以更好地了解用户和物品评论文档的 aspect 重要性，以及 (4) 预测层来估计用户对物品的评分。

所提出的方法的主要贡献可以强调如下：

- 我们提出了一种情感感知深度推荐系统，该系统通过交互式注意力机制将半监督主题模型整合到深度学习技术中，以获得更好的推荐系统的性能。
- 我们设计了一个半监督主题模型来提取特定领域的 aspect 和相关的用户情感词典，以进行有效的情感感知用户/物品表示学习。
- 我们设计了一个共同注意机制，以更好地学习细粒度的用户-物品交互。
- 我们对可公开访问的数据集进行了一系列实验，以评估所提出模型相对于基线方法的性能。

本文的其余部分安排如下：第 II 节和第 III 节分别介绍了相关工作和所提出方法的概述。 第四部分描述了实验研究，最后，第五部分总结了论文。

## 2 相关工作

本节回顾与我们提出的方法特别相关的不同方法。 这包括基于主题的推荐系统、基于深度学习的推荐系统和基于注意力的推荐系统。在以下小节中，我们将回顾这些类别中的每一个。

### A. 基于主题的推荐系统

在过去的几年中，已经提出了几种主题建模方法来合并细粒度信息以进行更准确的评分预测。 这些方法中的大多数使用来自用户评论的潜在主题，其他方法基于分解机学习潜在因素。 例如，[4] 和 [5] 使用主题建模将潜在因子与基于定义的转换过程的潜在主题相结合。[15] 和 [16] 将潜在主题与潜在因素对齐，以生成用于评级建模的用户和物品的潜在表示。 参考文献 [6] 应用主题建模从用户文本评论中学习特征，使用高斯方法进行评分预测。 参考文献[17]引入了一种联合模型来联合利用 aspect 评级和用户情绪来缓解冷启动问题

上述所有方法通常基于用于评分预测的传统（潜在狄利克雷分配）LDA模型[18]，并且LDA模型通常使用词袋方法。 因此，它们无法有效地捕获单词的上下文信息。 在本文中，与上述方法不同，我们设计了一个新的半监督主题模型来学习特定领域的 aspect  和情感词典，以更好地学习用户/物品表示。

### B. 基于深度学习的推荐系统

随着深度学习方法最近在计算机视觉和自然语言处理等各种应用中的成功[19]。 已经提出了许多方法来利用推荐系统的深度学习技术。这些包括去噪自动编码器 [10]、[20]、卷积神经网络 (CNN) [1]、[9]、[12]、[13] 和循环神经网络 (RNN) [21]-[24]  ]。 具体来说，吴等人[25] 和 Sedhain 等人[26] 利用去噪-自动编码器进行评分预测。 这种方法本质上存在数据稀疏问题。 为了解决这个问题，Wang 等人 [10]通过将概率主题模型与协同过滤技术相结合，引入了协同深度学习（CDL）模型。 CDL 是 [7] 中引入的协同主题回归 (Collaborative Topic Regression, CTR) 方法的扩展变体。 由于它们的主题建模亲和力 (affinity)，这些模型忽略了单词的语义上下文。

由于其在图像处理和模式识别 aspect  的显着成功，CNN 模型已被广泛用于构建推荐系统。 特别是，郑等人 [1] 提出 Deep-CoNN 来利用两个 CNN 网络对用户和物品评论进行联合建模，以改进评分预测。Catherine 和 Cohen [13] 提出了一种称为 Transnet 的模型作为 Dee-pCoNN 的扩展。 作者使用了一个额外的顶层来改进评分预测。  [27]最近引入了类似的方法。 作者提出了一个可扩展的深度推荐系统，使用两个独立的 CNN 模型。

RNN 也被广泛用于各种应用中的推荐系统，如电影推荐 [14]、next basket 个性化推荐和新闻推荐 [28]。 特别是，Bansal 等人 [22] 介绍了一种基于门控循环单元 (GRU) 模型的多任务学习方法来编码文档以获得隐式用户反馈。 高等人 [29] 介绍了一种动态 RNN 框架，用于在统一框架中对用户的动态兴趣进行建模。 希达西等人 [24] 利用 RNN 开发基于 session 的推荐系统。

尽管它们取得了最先进的成功，但这些模型是有限的，因为它们特别以静态和独立的方式导出用户/物品潜在特征向量，而忽略了复杂的细粒度用户-物品交互。通过这种方式，物品和用户仅在顶层进行交互，其中学习的用户/物品表示用于整体评分预测。 因此，在这些模型中很难提供用户对物品评分背后的见解

### C. 基于注意力的推荐系统

注意机制的主要思想在直觉上类似于人类的视觉注意。特别是，它配备了一个神经网络，能够选择目标输入中最重要的部分，例如给定评论中的特定单词或图像中的特定区域。 这个想法已被有效地应用于许多应用，例如计算机视觉 [30]、机器翻译 [31] 和自然语言处理 [32]。 最近，神经注意力已被用于构建推荐系统 [12]、[21]、[33]。 例如，Bahdanau 等人 [31] 提出了一种基于注意力的方法来准确对齐编码器-解码器框架以进行机器翻译。 随着机器翻译中自我注意方法的实现，[34]和[35]利用自我注意来对用户行为进行建模。[36] 提出了一种用于视频/图像推荐的多级注意机制。 徐等人 [12] 提出了一个带注意力的 CNN 网络，并在顶层使用分解机进行评分预测。 作者设计了两种注意力机制，即局部和全局注意力，以更好地模型用户和物品表示。

我们的共同注意方法与 [37] 中使用的神经注意方法密切相关，后者被视为成对神经注意的一种形式。 作者将神经共同注意应用于视觉问题和答案。 与上述方法不同，我们提出的模型利用了两种不同的注意力机制，即综合神经注意力从用户文本评论中捕获信息量最大的 aspect 和相关的用户情绪，以及针对细粒度的用户-物品交互的神经协同注意力网络。

## 3 方法

本节描述了提议的 SDRA（具有神经注意力的情感感知深度推荐系统）模型，该模型利用半监督主题模型和具有神经注意力机制的 LSTM 编码器。 我们首先在表 1 中指定本文中使用的符号，并描述研究的问题设置。 然后详细阐述模型架构的概述和要优化的目标函数。

### A. 问题设定和符号表示

令 $D$ 为一组用户 $u$ 所写的给定物品集 $i$ 的语料库，并且每个评论 $d_{u,i} \in D$ 都伴随着总体评分 $r_{u,i}$，它显示了用户对物品的总体满意度。 每个物品-用户交互都可以表示为一个元组 

$(u, i,r_{u,i}, d_{u,i})$。
主要目标是计算用户 $u$ 对用户未观察到的物品 $i$ 的未知评分 $\hat r_{u,i}$ 。表 1 显示了本文中使用的符号。

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017160025536.png" alt="image-20211017160025536" style="zoom:80%;" />

<center>【表1 符号表示】</center>

### B. SDRA总览

所提出的 SDRA 模型的整体架构如图 1 所示。 它包括四个主要组件：

(1) LSTM 编码器，以更好地学习单词的语义和上下文信息。

(2) 用于提取特定领域 aspect  和情感词法图标的半监督主题建模。

(3) 用于估计用户和物品 aspect  重要性的共同注意网络层。

(4) 评分预测层，用于估计预测评分。 该模型的详细描述在以下小节中介绍。

![image-20211017160639667](https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017160639667.png)

<center>【图1 模型架构】</center>

### C. 嵌入层 (embedding layer)

嵌入层从文档 D 中获取一组序列词并将它们映射到 $n$ 维矩阵 $x_i \in \mathbb R^n$ 。这种表示技术用于对单词携带的语义和句法信息进行编码。 嵌入层可以使用预训练的词向量（例如 Glove 或 Word2vec）进行初始化。 在本文中，在大量谷歌新闻语料上预训练的 Word2vec [38] 用于初始化嵌入层。

### D. 序列编码层

序列编码层的主要目的是提供输入词序列的上下文注释 (annotation) 。这里采用 LSTM 模型，因为它在很多应用中表现良好，并已成功用于序列建模 [39]。 与许多 RNN 变体一样，LSTM 模型由神经网络中的链重复组件组成。LSTM 不是将重复组件分配为一个简单的结构，而是使用其信息可以更新的单元状态。 形式上，给定时间步 $t$ 的输入 $x$，先前的单元状态 $c_t$ 和当前的隐藏状态 $h_t$ 可以更新如下：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017161726422.png" alt="image-20211017161726422" style="zoom: 50%;" />

其中，$W$ 和 $U$ 表示要学习的权重矩阵，$i_t$，$f_t$ ，$O_t$ ，$\hat c_t$ 分别是输入门、遗忘门、输出门和记忆单元，$\partial$ 和 $\odot$ 分别是 sigmoid 函数和元素乘法。  LSTM 的输出是隐藏状态序列 $(h_1, h_2 . . .h_t) \in \mathbb R^d $获得的。每个注释都包含有关整个评论的信息，重点关注第 $i$ 个单词。 其中 $d$ 是 LSTM 编码器的隐藏状态的向量维度大小

### E 半监督主题建模

为了更好地捕捉特定领域的 aspect  和相关联的用户情绪词典，设计了一个半监督的主题模型。 受 [40] 的启发，我们扩展了 vanilla LDA 模型 [18]，利用种子词 (seed words) 来指导主题模型设计。主题在相关的 aspect  和情感词典类别中自动提取和分类情感词和 aspect  term。
我们假设每条评论都包含两类主题：$k$ 个具有两种极性（负面和正面情绪）的 Sentiment 主题和 $m$ 个 aspect 主题。 每个主题都与词的多项分布相关。 假设语料库词汇 c 包含由 $1...\cal U$ 索引的 $\cal U$ 个不同单词。对于每个评论，我们得到两个主题分布 $\Phi^s$ 、$\Phi^a$，分别代表情感主题和 aspect 主题的概率分布。 设 $\Phi^s_{k,w}$ 和 $\Phi^a_{m,w}$ 分别表示词 $w$ 在情感主题 $k$ 和 aspect  $m$ 下的概率。对于每个情感主题 $k$，其词分布 $\Phi^s_k$ 是从 ${\rm Dir}(\Omega_k^s)$ 中选择的，其中 $\Omega_k^s$ 是一个 $\cal U$ 维向量。  它的计算如下：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017165357264.png" alt="image-20211017165357264" style="zoom:80%;" />

其中 $\gamma_0$ 和 $\gamma_1$ 是模型的超参数。 如果词 $w$ 是情感主题 $k$ 中的种子词，则 $\beta_w = 1$，否则 $\beta_w= 0$。理想情况下，来自主题 $k$ 的种子词由有偏 (biased) 先验 $\Omega_k^s$ 强制执行。相反， aspect 主题分布 $ \Omega_m^a \sim {\rm Dirichlet}(\Omega_m^a)$ 以类似的方式构建。主题模型的生成方法可以在下面重点介绍。 该模型的图形表示如图2所示：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017165910995.png" alt="image-20211017165910995" style="zoom: 80%;" />

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017165937993.png" alt="image-20211017165937993" style="zoom:80%;" />

<center>【图2】</center>

为了估计未知参数，本文使用了吉布斯采样算法[41]。 由于篇幅所限，有兴趣的读者可以参考[41]中的 Gib 采样方法。获取 sentiment 和 aspect 主题的词分布，形式上的 $\Phi^s$ 和 $\Phi^a$ 可以按如下获得：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017170230613.png" alt="image-20211017170230613" style="zoom:67%;" />

其中 $N^s_{k,w}$ 和 $N^a_{m,w}$ 表示文档中词 $w$ 的情感和 aspect  主题的出现次数。 读者可以参考 [41] 了解采样程序的详细信息。

接下来，通过全连接层将情感主题 $i$ 和 aspect 主题 $j$ 的 $\cal U$ 维词分布转换为低维情感嵌入 $e^s$ 和 aspect  嵌入 $e^a$，以获得与 LSTM 隐藏状态相同的维度。 因此，我们有：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017170525961.png" alt="image-20211017170525961" style="zoom: 50%;" />

其中矩阵 $W^s \in \mathbb R^{\cal U×d}$ 和  $W^a \in \mathbb R^{\cal U×d}$  是可训练参数，$d$ 是 LSTM 隐藏状态的维度，tanh 是非线性函数。

### F. 交互式主题注意力

交互式注意力机制基本上旨在从输入文本评论中捕获最相关的信息，以学习 aspect  / 情感感知文档表示。形式上，给定来自 LSTM 编码器的隐藏状态 $[h_1, h_2...h_t]$ 以及情感嵌入 $[e^s_1 , e^s_2...e^s_n ]$ 和 aspect  嵌入$[e^a_1 , e^a_2...e^a_n ]$ 从主题建模中学习，交互式注意力网络分别生成情感词典和 aspect  特定的注意力权重，如下所示：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017171033849.png" alt="image-20211017171033849" style="zoom:67%;" />

其中 $\alpha^s_t$ 和  $\alpha^a_t$  是注意力分数，表示相关的隐藏状态 $h_t$ 分别作为 aspect 和情感指标的可能性。其中 $\gamma$ 是表示隐藏状态 $h_t$ 在上下文中的重要性的得分函数。$\gamma$ 分数可以定义为：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017171206524.png" alt="image-20211017171206524" style="zoom:67%;" />

其中 $U^T_s 、U^T_a 、W^s$ 和 $W^a$ 是要学习的投影参数，$bs$ 和 $ba$ 是偏差。 在计算情感词和 aspect 词的交互式注意力向量后，可以得到情感感知和 aspect  特定表示如下：
$$
V^s_u = \sum^k_{t=1} \alpha^s_t h_t
\tag{14}
$$

$$
V^a_u = \sum^N_{t=1} \alpha^a_t h_t 
\tag{15}
$$

为了最终获得用户评论文档表示的 Aspect/情感感知，可以按如下方式连接词典和基于 aspect  的表示：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017171602643.png" alt="image-20211017171602643" style="zoom:67%;" />

其中 $W_s$ 和 $W_a$ 是投影参数，$\rho \in [0, 1]$ 用于控制情绪和 aspect 感知表示的效果。相反，可以用类似的方式获得物品表示 $\partial_i$。 $\{\empty_u，\partial_i\}$，可以被获得然后用于评分预测任务。 通过这种方式，感知到了用户和物品的 aspect  / 情感。

### G. 共同注意力网络层

直观地说，用户对 aspect 的偏好可能会因所考虑的物品而异。因此，我们的目标不是具有静态的用户/物品重要性，而是通过计算每个用户-物品对的 aspect  重要性来在单词级别对动态细粒度的用户-物品交互进行建模。 因此，受 [37] 工作的启发，我们引入了一种共同注意机制来捕获用户和物品不同 aspect  之间的相对重要性，以更好地学习交互式用户物品表示。

具体来说，将用户表征作为上下文来学习物品 aspect  的重要性，同样地，将物品表征作为上下文来学习用户 aspect 的重要性。 此操作的输出将是一个 $K$ 维向量，显示每个 aspect 相对于物品相关用户情绪的重要性，以及用户的相应向量。 共同注意机制的概述结构如图 3 所示。

为了实现动态细粒度的用户-物品交互，需要计算目标用户和物品之间的相似度。 具体来说，给定用户表示 $\empty_u \in \mathbb R ^{k\times l_1}$ 和物品表示 $ϑ_i \in \mathbb R ^{k\times l_1}$ ，亲和矩阵 $C \in \mathbb R ^{k×k}$ 计算如下： 

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017172616844.png" alt="image-20211017172616844" style="zoom:67%;" />

其中 $W_c \in \mathbb R^{l_1×l_1}$ ，是要学习的权重矩阵。
按照[37]，我们应用矩阵 $C \in \mathbb R^{k×k}$ 作为特征来计算用户和物品的重要性如下： 

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017172733638.png" alt="image-20211017172733638" style="zoom:67%;" />

其中 $W_y, W_z \in \mathbb R ^{l_1×l_2}$ 和 $V_y, V_z \in \mathbb R^{l_2}$ 是要学习的权重参数。 $ \mathbb Q_u \in \mathbb R^k $ 和 $\mathbb Q_i \in \mathbb R^k$ 分别是用户和物品的重要性。
这样就可以得到用户和物品的重要性，$(\mathbb Q_u, \mathbb Q_i)$ 用于评分预测层

### H. 评分预测层

评分预测层是推荐过程的实际评分预测任务发生的地方。预测层通常接受用户/项目表示 $(\empty_u，ϑ_i)$ 和方面重要性 $(\mathbb Q_u, \mathbb Q_i)$ 作为输入，并将它们传递给分解机。分解机接受实值特征向量并处理成对交互。 因此，总体评级可以推断为：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017173151717.png" alt="image-20211017173151717" style="zoom:67%;" />

其中 $b_u、b_i$ 和 $µ$ 分别是用户、项目和全局偏置。可以使用反向传播方法来学习模型参数，并使用均方误差函数作为损失函数：

<img src="https://gitee.com/Wales-Z/image_bed/raw/master/img/image-20211017173243592.png" alt="image-20211017173243592" style="zoom:67%;" />

其中 $r_{u,i}$ 是评论 $D$ 中观察到的评分，而 $\hat r_{u,i}$ 是未观察到的评分。

