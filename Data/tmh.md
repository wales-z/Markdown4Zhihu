# Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain Collaborative Filtering with Text

## 摘要

协同过滤 (CF) 是推荐系统 (RS) 的关键技术。  CF 仅利用用户-物品行为交互（例如，点击），因此存在数据稀疏问题。 一个研究方向是整合辅助信息，如产品评论和新闻标题，从而产生混合过滤方法。 另一个线程是从其他源领域迁移知识，例如用书籍领域的知识改进电影推荐，从而产生迁移学习方法。 在现实生活中，没有一种服务可以满足用户的所有信息需求。 因此，这激励我们在本文中利用 RS 的辅助信息和源信息。 我们提出了一种新颖的神经模型，以端到端的方式平滑地启用Transfer meeting Hybrid（TMH）方法，用于非结构化文本的跨域推荐。TMH 通过记忆 (memory) 模块从非结构化文本中专注地提取有用的内容，并通过迁移网络从源域中选择性地迁移知识。 在两个真实世界的数据集上，通过与各种基线进行比较，TMH 在三个排名指标方面表现出更好的性能。 我们进行了彻底的分析，以了解文本内容和迁移的知识如何帮助提出的模型。

# 1 介绍

推荐系统广泛应用于各个领域和电子商务平台，例如帮助消费者在亚马逊上购买产品、在 Youtube 上观看视频以及在 Google 新闻上阅读文章。 协同过滤 (CF) 是基于简单直觉的最有效方法之一，即如果用户过去对物品进行了相似的评分，那么他们将来可能会对物品进行相似的评分。 可以学习用户和物品的潜在因素的矩阵分解 (MF) 技术是其主要基石 [25, 35]。最近，像多层感知器（MLP）这样的神经网络被用来从数据中学习交互函数 [10, 17]。  MF 和 neural CF 面临数据稀疏和冷启动问题。

一种解决方案是将 CF 与内容信息集成，从而导致混合方法。 物品通常与内容信息相关联，例如非结构化文本，如新闻文章和产品评论。 这些额外的信息源可以缓解稀疏性问题，对于用户-物品交互数据之外的推荐至关重要。 对于推荐研究论文和新闻文章等应用领域，与物品相关的非结构化文本是其文本内容 [1, 51]。 其他领域，如推荐产品，与物品相关的非结构化文本是其用户评论，它证明了消费者的评级行为 [33, 61]。 已经提出了主题建模和神经网络来利用物品内容并提高性能。 记忆网络广泛用于问答和阅读理解以进行推理[46]。 记忆可以自然地用于对诸如物品内容 [19] 之类的其他来源进行建模，或者对与该用户一起消费共同物品的用户社区进行建模 [11]。

另一种解决方案是从相关领域迁移知识，跨域推荐技术解决了此类问题[3,27,39]。 在现实生活中，用户通常会参与多个系统来获取不同的信息服务。 例如，用户在应用商店中安装应用程序并同时从网站上阅读新闻。 它为我们提供了通过跨域学习来提高目标服务（或所有服务）的推荐性能的机会。 按照上面的示例，我们可以使用二进制矩阵表示应用程序安装反馈，其中条目表示用户是否安装了应用程序。 类似地，我们使用另一个二元矩阵来指示用户是否阅读过新闻文章。 通常这两个矩阵是高度稀疏的，同时学习它们是有益的。 这个想法被强化为集体矩阵分解（CMF）[45]方法，该方法通过共享用户潜在因素来联合分解这两个矩阵。 它结合了目标域上的 CF 和辅助域上的另一个 CF，实现了知识迁移 [38, 59]。 在神经网络方面，给定来自两个任务的两个激活图，cross-stitch 卷积网络 (CSN) [34] 及其稀疏变体 [21] 学习两个输入激活的线性组合并将这些组合作为输入提供给连续层的过滤器，从而实现两个领域之间的知识迁移。

这两个线程促使我们在本文中利用来自 RS 的内容和跨域信息的信息。 为了捕获文本内容并迁移跨域知识，我们提出了一种新颖的神经模型 TMH，用于以端到端的方式对非结构化文本进行跨域推荐。  TMH 可以通过记忆网络 (MNet) 专注地提取有用的内容，并且可以通过迁移网络 (TNet)，一种新颖的网络，选择性地跨域迁移知识。 一个共享的特征交互层堆叠在顶部，以耦合从单个网络学习的高级表示。 在真实世界的数据集上，TMH 通过与各种基线进行比较，在排名指标方面表现出更好的性能。 我们进行彻底的分析，以了解内容和迁移的知识如何帮助 TMH。

据我们所知，TMH 是第一个在端到端学习中使用非结构化文本迁移跨域知识以进行推荐的深度模型。 我们的贡献总结如下：

- 提出的TMH 利用文本内容并使用以端到端方式训练的注意力机制迁移源域。 它是第一个使用基于注意力的神经网络将跨领域知识迁移到非结构化文本推荐的深度模型。
- 我们解释记忆网络以专注地利用文本内容来将单词语义与用户偏好相匹配。它是最近采用记忆网络进行混合推荐的一些工作之一。
- 迁移组件可以通过注意力权重在目标用户-物品交互的指导下有选择地迁移源物品。 它是一种新颖的跨域推荐迁移网络。
- 所提出的模型可以缓解包括冷用户和冷物品启动在内的稀疏问题，并且在两个真实世界数据集的排名指标方面优于各种基线。

本文的结构如下。 我们首先在第 3 节中介绍问题公式。然后我们分别在第 5.1 节中介绍了利用文本内容的记忆组件和在第 5.2 节中传递跨域知识的迁移组件。 我们在第 5 节中提出了用于非结构化文本跨域推荐的神经模型 TMH，然后是其模型学习（第 5.4 节）和复杂性分析（第 5.5 节）。在第 6 节中，我们通过实验证明了所提出模型在各种基线上的优越性能（第 6.2 节）。我们在第 6.3 节中展示了所提出模型的知识转移和文本内容的好处。 我们可以减少难以准确预测的冷用户和冷物品的数量（第 6.4 节），从而缓解冷启动问题。 我们在第 2 节回顾相关工作并在第 7 节总结论文。

## 2 相关工作

暂略

## 3 问题形式化

对于带有隐式反馈的协同过滤，有一个二元矩阵 ${\rm R} \in {\mathbb R}^{m×n}$ 来描述用户-物品交互，其中如果用户 $u$ 与物品 $i$ 有交互，则每个 $r_{ui} \in \{0, 1\}$ 为 1（称为观察条目），否则（未观察到）0为否则。

用 $\cal U$ 表示大小为 $m$ 的用户集和，用 $\cal I$ 表示 $n$ 个物品的集合。通常交互矩阵非常稀疏，因为用户 $u \in \cal U$ 只占据所有物品的一个非常小的子集。 类似的物品推荐任务，每个用户只对识别 top-K 的物品感兴趣。 这些物品按其预测分数排名：
$$
\hat r_{ui}=f(u,i|\Theta)
\tag1
$$
其中  $f$ 是交互函数，$\Theta$ 表示模型参数。

对于基于 MF 的 CF 方法，交互函数 $f$ 是固定的，并通过用户和物品向量之间的点积计算。对于neural CF，神经网络用于参数化函数 $f$ 并从交互数据中学习它（参见第 4 节）：
$$
f(x_{ui}| P, Q, \theta_f) = \phi_o (...(\phi_1(x_{ui}))...)
\tag2
$$
其中输入 $x_{ui}=[P^T x_u, Q^T x_i] \in {\mathbb R}^{2d}$ 是由用户和物品的embedding通过连接(concatenate) 得到的，而这里的embedding是 embedding 矩阵 $P \in {\mathbb R}^{m \times d}$  和 $Q \in {\mathbb R}^{n \times d}$ 中对应用户/物品的独热编码的投影。输出层和隐藏层由神经网络中的 $\phi_o$ 和 $\{\phi_l\}$ 计算。







## 5 TMH 模型

### 5.2 选择要迁移的源域物品

我们引入了一个迁移组件来利用源领域知识。 用户可能会参与多个系统来获取不同的信息需求，例如用户在应用商店安装应用程序和阅读其他网站的新闻。 跨域推荐 [3] 是缓解稀疏问题的有效技术，其中迁移学习（包括多任务学习）[4, 38, 59] 是一类底层方法。 典型的方法包括 collective 矩阵分解 (CMF) [45] 方法，该方法通过共享用户潜在因素来联合分解两个评分矩阵，从而实现知识转移。 cross-stitch 网络 [34] 及其稀疏变体 [21] 使每个域的两个基础网络之间能够以深入的方式共享信息。 这些方法将知识转移视为一个全局过程（共享全局参数），并且不会将源项与给定用户的特定目标项进行匹配。

我们提出了一种新颖的迁移网络（TNet），它可以选择性地迁移特定目标物品的源知识。 由于物品之间的关系对于提高单个域的推荐性能 [26、36、37、41] 很重要，因此我们希望捕获用户的目标物品和源物品之间的关系。 中心思想是在知识转移期间学习特定于给定目标物品的源物品的自适应权重

给定用户 $u$ 交互过的源域物品集合 $[j]^u=(j_1,j_2,...,j_s)$  ，Tnet 学习一个迁移向量 $c_{ui} \in {\mathbb R}^d$ 来捕获所给定的用户 $u$ 的源域物品与目标域物品 $i$ 的关系。通过从书籍领域迁移知识来改进电影推荐的示例可以说明潜在的观察结果。 当我们预测用户对电影《指环王》的偏好时，她读过的《霍比特人》、《精灵宝钻》等书籍的重要性可能远高于《Call Me by Your Name》等 。

物品 $i$ 与源域物品的相似度可以用它们的点积来计算：
$$
a_j^{(i)}= x_i^T x_j, j=1,...,s
\tag{11}
$$
其中 $x_j \in {\mathbb R^d}$ 是embedding矩阵 $H \in {\mathbb R}^{n_S \times d}$ 中源域物品 $j$ 的embedding。这个分数计算了目标物品与用户消费的源物品之间的兼容性。例如，目标电影 $i$ = 《指环王》与源书 $j$ = 《霍比特人》的相似度可能大于与源书 $j$  = 《请以你的名字呼唤我》的相似度（给定一个用户 $u$)。

我们将相似度归一化为源物品的概率分布：
$$
\alpha_j^{(i)} = Softmax(a_j^{(i)})
\tag{12}
$$
然后迁移向量是相应的源域物品embedding的加权和：
$$
c_{ui} = ReLU(\sum_j \alpha_j^{(i)}x_j)
\tag{13}
$$
我们通过激活函数整流线性单元（ReLU）在传递向量上引入非线性。根据经验我们发现激活函数 $ReLU(x) = max(0,x)$ 由于其非饱和性和对稀疏数据的适用性而表现良好。转移向量 $c_{ui}$ 是一种高级表示，将来自源域的知识总结为 TNet 的输出。  TNet 可以在目标的用户-物品交互的指导下，从源物品的相应 embedding 中选择性地迁移表示。

### 5.3 TMH

所提出的 TMH 模型的架构在图 1 中作为前馈神经网络 (FFNN) 进行了说明。输入层指定用户 $u$、目标物品 $i$ 和相应的源物品 $[j]^u = (j_1, ..., j_s )$ 的embedding。内容文本 $d_{ui}$ 由 MNet 中的记忆建模以生成高级表示 $o_{ui}$ 。 在 TNet 中的 $c(u,i)$ 的指导下，将源物品迁移到迁移向量 $c_{ui}$ 中。这些计算路径分别在上面的5.1节和5.2节中介绍。

首先，我们使用一个简单的neural CF 模型（CFNet），它有一个隐藏层来学习用户-物品交互的非线性表示：
$$
Z_{ui} = ReLU(Wx_{ui}+b)
\tag{14}
$$
其中 $W$ 和 $b$ 是隐藏层的权重和偏置参数。通常，在典型的塔式架构中，$z_{ui}$ 的size是 $x_{ui}$ 的一半。

三个独立网络的输出可以查看内容文本、源领域知识和用户-物品交互的高级特征。 它们来自不同网络学习的不同特征空间。 因此，我们在所有特征的顶部使用一个共享层：
$$
\hat r_{ui}=\frac{1}
{1+\exp(-h^T y_{ui})}
\tag{15}
$$
其中 $h$ 是共享层的权重参数，并且以下联合表示
$$
y_{ui}=[W_o o_{ui}, W_z z_{ui}, W_c c_{ui}]
\tag{16}
$$
是由从单个网络的线性映射输出连接起来得到的，其中矩阵Wo、Wz、Wc 是相应的线性映射变换。

### 5.4 学习

由于隐式反馈的性质和物品推荐的任务，平方损失 $(\hat r_{ui} -r_{ui})^2$ 可能不适合，因为它通常用于评分测。 相反，我们采用二元交叉熵损失。

